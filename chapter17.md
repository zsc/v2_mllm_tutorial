# [chapter17.md] 模型架构：Qwen‑式自回归 Transformer（Dense / 先进 MoE，早期融合）

### 1. 开篇段落

在第 16 章，我们穿越了生成-理解一体架构的演化历史，从理论层面探讨了统一多模态模型的可能性。现在，我们将从高远的理论天空降落到坚实的工程地面。本章是整个项目的技术心脏，我们将在这里绘制出 VLA 大模型的精确蓝图。这不仅是一份参数列表，更是我们团队关于模型能力、计算预算和实现路径的核心共识。本章的学习目标是：深刻理解为何选择 Qwen 式自回归 Transformer 作为统一骨架；精通 1B Dense 与 10B MoE 两档规模的架构细节、参数权衡与设计哲；并掌握多模态早期融合、**先进 MoE** 路由机制、**Perceiver 式视觉压缩**以及 VLA 动作头等关键模块的实现原理。本章的产出——冻结的架构方案，将成为指导后续所有数据处理、基建搭建和训练策略的“根本大法”。**[里程碑 W7]** 的达成，标志着我们从“想做什么”进入了“具体怎么做”的阶段。

### 2. 文字论述

#### 17.1 核心思想：以语言为中心的统一生成框架

我们的终极目标是构建一个能够理解复杂的、动态的多模态世界，并能在其中自主决策与行动的智能体。实现这一目标的最佳路径，是采用一个具有极致扩展性的统一框架。我们坚信，**自回归（Autoregressive, AR）Transformer** 是当前实现该目标的最优解。

其核心哲学在于“**万物皆可 Token 化**”。无论是“你好”这两个汉字、一段语音的声学特征、一幅图像的视觉概念，还是一个“方向盘左转 15 度”的驾指令，都可以被编码为离散的 token 序列。一旦完成这一转换，所有看似迥异的任务都统一为同一个、极其简单的目标：

$P(\mathbf{x}) = \prod_{i=1}^{N} P(x_i | x_{<i}, \mathbf{c})$

其中，$\mathbf{c}$ 是由文本、音频、视频、3D 等任意模态组成的庞大上下文，而 $\mathbf{x}$ 则是模型需要生成的目标序列（一段描述、一个回答、一连串动作指令）。这种范式的优雅之处在于：
1.  **生成与理解一体**：模型在预测下一个 token 时，必须深刻“理解”之前的多模态上下文。因此，强大的生成能力内生了强大的理解能力。
2.  **任务无关性**：VQA、图像描述、ASR、自动驾驶决策……所有任务都被拉平到同一个自回归预测的框架下，无需为每个任务设计复杂的独立模型头或损失函数。
3.  **无缝扩展**：未来若要加入新的模态（如雷达、触觉），我们只需设计相应的 Tokenizer 和投影器，将其融入下文 $\mathbf{c}$ 即可，主干网络无需改动。

我们选择 **Qwen 系列模型**的架构作为基座，因为它集成了近年来被广泛验证为高效且稳定的设计“全家桶”：**RMSNorm** 提供了比 LayerNorm 更优的稳定性和计算效率；**SwiGLU** 激活函数在 FFN 层中展现出比 ReLU/GeLU 更强的性能；**旋转位置编码（RoPE）** 解决了长序列建模的难题。这些组件共同构成了一个鲁棒、高性能的 Transformer 骨架。

#### 17.2 1B 与 10B 架构参数表 [里程碑 W7]

我们采取“两步走”策略：1B Dense 模型作为“侦察兵”，用于快速趟平数据、训练、评测全链路的各种坑；10B MoE 模型则是我们的“主战坦克”，承载着冲击业界顶尖性能的使命。

| 参数 (Parameter) | 1B (Dense) 参考值 | 10B (MoE) 参考值 | 设计 rationale / 权衡考量 |
| :--- | :--- | :--- | :--- |
| **总参数量** | ~1.3B | ~12B (激活 ~2B) | MoE 以存储换计算。总参数量巨大，但单样本前向传播仅激活一小部分（约 2B），实现“大容量、低成本”推理。|
| **模型类型** | Dense | Mixture-of-Experts (MoE) | 1B 验证全流程，成本可控。10B 追求知识广度与深度，MoE 是在给定算力下最大化模型容量的关键技术。|
| **层数 (Layers, `n_layer`)** | 24 | 32 | 更深的模型倾向于学习到更抽象的层次化特征，这对于理解复杂的 VLA 场景至关重要。10B 模型有能力支撑更深的结构。|
| **隐藏层维度 (`d_model`)** | 2048 | 4096 | 更宽的隐藏层为多模态特征的融合提供了更丰富的“信息总线”，减少信息瓶颈。|
| **注意力头数 (`n_head`)** | 16 | 32 | 与 `d_model` 等比例增长，确保每个头的维度（`d_head` = 128）不变。这有助于模型从不同子空间捕获特征。可考虑 GQA/MQA 优化推理。|
| **FFN 中间层比例** | 8/3 (e.g., `d_ff`=5461) | N/A (见 MoE) | SwiGLU FFN 的中间层维度通常是 `(2/3) * 4 * d_model`。这是一个被践证明的经验值。|
| **MoE 专家数 (`num_experts`)** | N/A | 16 | 16 是一个常见的权衡点。专家数太少，MoE 优势不明显；太多则导致路由难度和通信开销剧增。在 256 H100 集群上，可方便地按节点或 Pod 进行专家并行。|
| **MoE 激活专家数 (`top_k`)** | N/A | 2 | Top-2 路由相比 Top-1 能显著提升性能和训练稳定性，而计算量仅翻倍。Top-4 收益递减，成本更高。部署时可切换为 Top-1 以加速。|
| **上下文长度** | 32,768 tokens | 65,536 tokens | 这是产品能力的核心指标。32k 已能处理多数长文档和短视频，而 64k 则为处理多分钟的长视频、复杂的驾驶场景和维持长期记忆提供了可能。|
| **位置编码** | RoPE (Rotary) | RoPE (Rotary) | 对于数万 token 的长上下文，RoPE 的优秀外推性和稳定性是其成为不二之选的核心原因。|
| **归一化** | RMSNorm (Pre-Norm) | RMSNorm (Pre-Norm) | Pre-Norm 结构有助于梯度稳定，是训练深度 Transformer 的关键。RMSNorm 相比 LayerNorm 计算量更小。|
| **激活函数** | SwiGLU | SwiGLU | Gated Linear Unit (GLU) 的一种变体，通过门控机制动态调整信息流，被证明在多种任务上优于传统激活函数。|

#### 17.3 多模态早期融合 (Early Fusion) 与投影器

“早期融合”意味着所有模态在进入 Transformer 主体网络的第一层之前，就已经被转换到同一个高维语义空间中。这使得模型能够从最底层就开始进行跨模态信息的深度交互与对齐。

```ascii
      +-----------------+      +-----------------+      +---------------------+
Text  |  Text Tokenizer | ---> |   Embedding     | --+  |                     |
      +-----------------+      +-----------------+   |  |                     |
                                                     v  |                     |
      +-----------------+      +-----------------+      |                     |      +-----------------+
Image/|  Patch Encoder  | ---> | Vision Projector| ---> |   Transformer       |      |  LM Head /      |
Video | (e.g., ViT-L)   |      | (Perceiver-like)| --+  |   Backbone          | ---> |  Action Head    |
      +-----------------+      +-----------------+   |  | (1B Dense / 10B MoE)|      +-----------------+
                                                     v  |                     |
      +-----------------+      +-----------------+      |                     |
Audio |  Audio Codec    | ---> | Audio Projector | --+  |                     |
      |    Tokens       |      |  (Embedding)    |      |                     |
      +-----------------+      +-----------------+      +---------------------+
```

*   **视觉投影器 (Vision Projector) - Perceiver 式压缩是关键**
    一段 6-cam @ 480p @ 12Hz 的 10 秒视频会产生巨量的视觉 token，直接输入 Transformer 会导致序列过长，计算爆炸。为此，我们采用 **Perceiver 式的重采样/压缩** 机制：
    1.  **分块与编码**：视频帧被切分为重叠的 patches，每个 patch 经过一个小型视觉编码器（如 ViT-L 的一部分）得到一个 embedding。此时，我们会得到一个极其庞大的视觉 token 序列。
    2.  **时空嵌入**：为每个视觉 token 添加**可学习的嵌入向量**，以编码其来源信息：相机 ID (1-6)、时间戳、在图像中的 2D 位置。这是模型理解多摄环视时空关系的基础。
    3.  **交叉注意力压缩**：引入一小组（例如 256 个）可学习的**“潜查询” (latent query) 向量**。这些 query 通过交叉注意力机制去“审视”和“提炼”海量的原始视觉 token，将其中最重要的信息压缩并汇总到这 256 个 query 的 embedding 中。
    4.  **输出**：最终，无论原始视频多长，视觉投影器都稳定输出一个长度为 256 的 token 序列，送入主干网络。这极大地降低了计算复杂度。

*   **音频投影器 (Audio Projector)**
    由于我们采用了神经音频编解码器，音频已被转化为离散 token。这里的投影器相对简单，主要是一个标准的 Embedding 层，将离散的音频 token ID 映射为 `d_model` 维的向量。IPA token 序列可以作为并行的另一路输入，通过另一个 Embedding 层后与音频 token embedding 相加或拼接，为模型提供音素级别的先验知识。

*   **3D 投影器 (3D Projector)**
    遵循第 9 章的优先级：
    1.  **程序化脚本/X3D**：这些本身就是文本，直接通过扩展后的文本 Tokenizer 处理。
    2.  **.obj 等网格格式**：将其语法（如 `v`, `vn`, `f`）和数值离散化后，也看作一个特殊的“语言”，通过 Embedding 层进行投影。

*   **最终组装**
    所有模た的 embedding 序列，连同特殊的边界 token（如 `<|vision_start|>`, `<|audio_end|>`），在序列维度上被拼接 (`torch.cat`) 起来，形成一个完整的、超长的多模态输入序列，最后送入 Transformer 主干网络。

#### 17.4 详解“先进 MoE”架构 (10B)

准的 MoE 已经强大，但“先进”体现在对路由策略、负载均衡和通信优化的精细打磨上，这些细节决定了训练的成败。

```ascii
          Input Token Embedding (x)
                      |
                      v
      +----------------------------------+
      | Gating Network (Router)          |
      | y = Softmax(Linear(LayerNorm(x)))| --
      +----------------------------------+   |--> 计算路由权重 g_1, g_2, ...
        |                |                   |
        v                v                   |
  [g_1], [g_2], ... [g_k] (Top-k weights)    |
        |                |                   |
+-----------+      +-----------+         +-----------+
| Expert 1  |      | Expert 2  |   ...   | Expert 16 |
|  FFN(x)   |      |  FFN(x)   |         |  FFN(x)   |
+-----------+      +-----------+         +-----------+
        |                |
        v                v
        \                /
         \              /
      Output(y) = sum_{i in TopK}(g_i * Expert_i(x))
```

*   **带噪声的 Top-k 路由 (Noisy Top-k Gating)**: 为提升探索性，防止路由策略过早固化，可以在路由器的 logits 上加入少量高斯噪声。$G(x) = \text{Softmax}(\text{Linear}(x) + \epsilon)$, where $\epsilon \sim \mathcal{N}(0, \sigma^2)$。这在训练早期尤其有效。
*   **专家容量 (Expert Capacity)**: 在硬件实现中，每个专家的计算缓冲区大小是固定的。如果某个专家被分配的 token 数超过其容量，多余的 token 就会被**丢弃 (dropped)**。这是一个重要的 trade-off。
    *   **容量因子 (Capacity Factor, C)**: 通常设置为 `C = (总 token 数 / 专家数) * 1.25`。因子 `1.25` 意味着我们为每个专家预留了 25% 的冗余容量，以应对路由不均。
    *   **监控丢弃率**是 MoE 训练的关键指标。持续的高丢弃率意味着负载均衡出了问题，模型容量被浪费。
*   **精细的负载均衡损失 ($\mathcal{L}_{\text{aux}}$)**: 我们采用结合了“利用”和“重要性”的辅助损失。
    *   **利用率**: 鼓励每个专家处理大致相同数量的 token。
    *   **重要性**: 鼓励路由权重 $g_i$ 在所有 token 上的总和也趋于均衡。
    $\mathcal{L}_{\text{aux}} = \alpha \cdot (\mathbf{f} \cdot \mathbf{P})$, 其中 $\mathbf{f}$ 是各专家接收 token 的比例向量，$\mathbf{P}$ 是各专家路由权重的总和向量。$\alpha$ 是需要仔细调整的超参数。
*   **专家并行与通信**：在 Megatron 框架下，专家并行 (EP) 与张量并行 (TP) 和流水线并行 (PP) 正交。通常，我们会将 TP 限制在单个节点内（利用 NVLink），而 EP 和 DP (数据并行) 会跨节点（利用 InfiniBand）。MoE 的 FFN 计算中，会触发两次 `All-to-All` 通信：一次将 token 从它们的原始 GPU 发送到其被路由到的专家所在 GPU，一次将计算结果发回。这是 MoE 训练的主要通信瓶颈。

#### 17.5 VLA 核心：连接“思想”与“行动”

模型的行动能力源于其生成特定动作 token 的能力。

*   **动作离散化与词表**：我们将自动驾驶和具身智能的连续动作空间（如转向、油门、机械臂关节角度）进行**量化 (Quantization)**，映射到词表中的离散 token。
    *   **示例**: 方向盘转角 `[-90°, 90°]` 可以被量化为 256 个 bin，每个 bin 对应一个 token，如 `<action_steer_bin_0>` 到 `<action_steer_bin_255>`。
    *   **权衡**: 粒度越细，控制越精确，但词表变大，模型学习难度增加；粒度越粗，学习简单，但控制精度下降。我们需要根据任务需求找到最佳平衡点。
*   **分层动作 (Hierarchical Actions)**：为了让模型具备规划能力，我们设计了分层动作空间。模型首先生成一个高层级的意图 token (e.g., `<maneuver_lane_change_left>`)，然后在此基础上生成一系列低层级的微操 token（转向、加速等）。这使得模型的行为更具可解释性。
*   **动作头 (Action Head)**：虽然理论上可以与语言模型头 (LM Head) 共享，但实践中，为动作 token 设立一个独立的、轻量级的线性预测头通常更稳定。这意味着模型的最终输出层有两个“出口”：一个预测通用词表，一个预测动作词表。训练时，根据下一个真实 token 的类型（是语言还是动作），选择对应的头计算损失。
    $ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{LM}} + \beta \cdot \mathcal{L}_{\text{Action}} $
    其中 $\beta$ 是平衡语言和动作学习的权重。

### 3. 本章小结

本章为我们的 VLA 大模型构建了详尽而坚实的架构基础。我们不仅确定了宏观路线，也深入到了每一个关键组件的设计细节中。

*   **核心架构**: 选定以 **Qwen 式自回归 Transformer** 为统一骨架，利用其强大的生成能力驱动多模态理解与行动。
*   **双规并行**: 冻结了 **1B Dense**（用于快速迭代）和 **10B 先进 MoE**（用于性能突破）两套模型的详细参数配置，明确了各自的设计哲学。
*   **多模态融合**: 设计了**早期融合**方案，特别是引入了 **Perceiver 式压缩**来高效处理海量的多摄视频输入，并通过时空嵌入保留关键的几何与时序信息。
*   **MoE 机制**: 深入剖析了“先进 MoE”的内涵，包括带噪声路由、专家容量、精细负载均衡损失等核心机制，并指出了其在分布式训练中的通信模式。
*   **VLA 实现**: 通过**动作离散化**、**分层动作设计**和独立的**动作头**，为模型赋予了将“思想”转化为具体“行动”的能力，打通了感知-语言-行动的闭环。

### 4. 常见陷阱与错误 (Gotchas)

1.  **MoE 路由坍塌 (Router Collapse)**:
    *   **现象**: 训练中，专家负载极不均衡，部分专家“门庭若市”，部分“门可罗雀”，模型有效容量锐减，损失停滞。
    *   **调试**: 监控专家 token 分布直方图和 token 丢弃率。检查路由器的梯度，确保量级正常。
    *   **预防措施**: 使用**路由器 Z-loss** 来惩罚过大的 router logits；从一个较小的负载均衡系数 `α` 开始，然后随训练逐步增大学习率。在混合精度训练中，确保路由器的计算在 FP32 下进行以保证数值稳定性。

2.  **多模态特征尺度不一 (Mismatched Modality Scales)**:
    *   **现象**: 训练初期损失曲线剧烈震荡或直接 NaN。通常是某一模态（如视觉）的 embedding 范数远大于其他模态，导致梯度爆炸。
    *   **调试**: 在送入 Transformer 主干前，打印并监控每个模态投影器输出的 embedding 的 L2 范数。
    *   **预防措施**: 对每个模态的输出序列**独立进行一次 RMSNorm**。这是在多模态融合前最简单有效的“尺度对齐”手段。

3.  **视觉压缩信息瓶颈 (Vision Compression Bottleneck)**:
    *   **现象**: 模型在需要精细视觉细节的任务上（如识别小物体、读取仪表盘）表现不佳，尽管整场景理解尚可。
    *   **调试**: 可能是 Perceiver 的潜查询数量太少（如 64 个），导致压缩过于激进，丢失了高频细节。
    *   **预防措施**: 将潜查询数量作为一个重要的超参数进行实验。可以从 256 个开始。另外，可以设计混合方案：保留一小部分原始的高分辨率 patch token，与压缩后的潜查询 token 一起送入模型。

4.  **MoE 通信瓶颈 (MoE Communication Bottleneck)**:
    *   **现象**: 实际训练吞吐（TFLOPs/GPU/s）远低于理论峰值，`nvidia-smi` 显示 GPU 利用率不高，但网络带宽占用很高。
    *   **调试**: 使用 `nsight-sys` 等工具剖析训练 step，确认时间是否主要消耗在 `All-to-All` 通信原语上。
    *   **预防措施**: 与 Infra 团队紧密协作，优化集群拓扑。**将专家尽可能地分组在通信最快的单元内**（例如，一个 8-H100 NVLink 节点内的 8 个专家），以最大化利用高带宽的 NVLink，减少对较慢的跨节点 InfiniBand 的依赖。

5.  **动作抖动与不稳定性 (Action Jitter & Instability)**:
    *   **现象**: 在模拟器中，模型生成的动作序列在相邻的几个 bin 之间高频切换，导致车辆或机械臂产生不平滑、抖动的行为。
    *   **调试**: 检查动作 token 序列的熵。高熵可能意味着模型对选择哪个动作 bin 没有信心。
    *   **预防措施**: 引入**动作平滑正则化损失**，惩罚连续时间步之间动作的大幅变化。另外，在 SFT 阶段，使用高质量的人类驾驶/操作数据进行微调，让模型学习平滑的控制策略。
