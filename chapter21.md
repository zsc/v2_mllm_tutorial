# [chapter21.md] 对齐与中期训练（Instruction/Mid-training/偏好）

## 21.0 开篇段落：从“博学”到“可用”的最后一公里

预训练阶段，我们的模型通过在 30T token 的海量多模态数据上进行自监督学习，构建了一个强大的世界模型。它能理解像素、声波、文字、几何结构之间的复杂关联，堪称“博学”。然而，这种“博学”是被动的，模型的目标函数仅仅是预测下一个 token。它并不知道人类的意图，不理解对话的合作原则，更不具备在自动驾驶等高风险场景中做出符合安全、伦理与法规的决策能力。

本章将聚焦于“对齐”（Alignment）与“中期训练”（Mid-training），这是连接“博学”与“可用”桥梁，是项目成败的“最后一公里”。我们将深入探讨如何将一个通用的基础模型（Base Model）雕琢成一个有用（Helpful）、诚实（Honest）且无害（Harmless）的智能体。我们将从指令微调（SFT）开始，教会模型理解并执行多模态指令；接着深入语音和行动这两个特殊领域的对齐策略；最后，我们将探讨如何通过直接偏好优化（DPO）等先进技术，让模型学习人类的复杂偏好，从而真正满足 VLA、自动驾驶和语音交互的生产级需求。

**学习目标**:
1.  深入理解多模态指令微调（SFT）的数据生态、格式设计哲学与规模化生产流程。
2.  掌握语音交互对齐中的延迟、打断、韵律和轮次管理等关键技术。
3.  系统学习具身行动对齐的范式，从行为克隆（BC）的陷阱到离线强化学习（Offline RL）的机遇。
4.  精通直接偏好优化（DPO）的数学原理与工程实践，并了解其在安全约束下的应，如 Constitutional AI。
5.  识别并规避对齐过程中的核心挑战，如灾难性遗忘、奖励“黑客”和对齐税。

---

## 21.1 指令数据构造与多模态对话格式

监督微调（Supervised Fine-Tuning, SFT）是模型对齐的第一步，其核心目标是教会模型以对话形式遵循指令。对于我们的 VLA 模型，这意味着指令和响应本身就是多模态的。

### 21.1.1 统一对话格式的哲学

我们将所有交互都抽象为一个统一的、结构化的对话序列。这种格式不仅是模型的输入/输出规范，更是我们注入领域知识、引导模型思考、增强其可解释性的主要手段。

```ascii
[SYSTEM]
<|system|> 你是一个多模T态驾驶助手，安全第一，遵守交通法规，并能与乘客进行语音交互。
[USER]
<|observation|>
  <|camera_pose id="1" pos="[x,y,z]" rot="[q_w,q_x,q_y,q_z]"|> <|video_cam1_t0|>...<|video_cam1_tN|>
  <|camera_pose id="2" pos="..." rot="..."|> <|video_cam2_t0|>...<|video_cam2_tN|>
  ...
  <|camera_pose id="6" pos="..." rot="..."|> <|video_cam6_t0|>...<|video_cam6_tN|>
  <|audio|> <|audio_t0|>...<|audio_tM|> (乘客的语音指令)
<|text|> "前方路况如何？感觉有点复杂，我应该怎么并进去？"
<|ipa|> "fɔŋ²¹⁴ tɕʰiɛn²¹⁴ lu⁵¹ kʰuaŋ⁵¹ ʐu³⁵ xɤ³⁵? kan³⁵ tɕyɛ³⁵ iou²¹⁴ tiɛn³⁵ fu⁵¹ tsa³⁵, uo²¹⁴ iŋ¹ kai¹ tsən³⁵ mə pɪŋ⁵¹ tɕɪn⁵¹ tɕʰy⁵¹?"
[ASSISTANT]
<|thought|>
  1.  **解析用户意图**: 用户感到不确定，询问前方路况并请求并线策略。
  2.  **感知分析**:
      -   **视频(Cam 1, 2)**: 前方主路车流量大，车速约 60 km/h。
      -   **视频(Cam 3, 4 - 侧方)**: 左侧车道后方 50 米处有一辆卡车正在快速接近，速度差较大，不宜立即并线。
      -   **视频(Cam 5 - 右后方)**: 右后方车道有足够空间。
      -   **语音分析**: 用户语气中带有轻微的焦虑情绪。
  3.  **决策推理**: 基于“安全第一”原，当前最佳策略是：减速等待，让卡车通过，然后寻找下一个安全的并线窗口。需要用安抚性的语言回应用户。
  4.  **行动规划**: 生成一个平滑的减速和车道保持的动作序列。
</|thought|>
<|text|> "好的，请放心。前方主路车流较快，特别是左后方有一辆卡车正在接近。为了安全，我们先稍微减速保持车道，等它通过后，我们再找机会汇入。"
<|action|>
  <|control type="lon" value="-0.5m/s^2" duration="3s"|>
  <|control type="lat" value="lane_keeping"|>
```

**设计要点深度解析**:
1.  **`<|thought|>` 思维链 (CoT)**: 这是提升模型复杂推理能力和可解释性的关键。在训练时，我们强制模型生成推理步骤。这不仅让模型学习了如何“思考”，也为我们调试模型提供了宝贵的中间过程。高质量的 CoT 数据通常需要由领域专家（如资深驾驶员、AI 科学家）撰写或审核。
2.  **多模态特殊 Token**:
    *   `<|camera_pose ...|>`: 明确注入相机外参，让模型理解多摄像头的空间关系，这是实现 3D 感知和环视理解的基础。
    *   `<|ipa|>`: 引入国际音标（IPA）标注，让模型不仅能识别文本，还能理解发音的细微差别，这对处理方言、少数语种和口音至关重要。
    *   `<|action|>`/`<|control|>`: 将模型的行动输出结构化、离散化。这比直接回归连续值更稳定，也更容易与车辆的控制模块接口。
3.  **数据生态系统**: 高质量SFT数据从何而来？
    *   **人类专家标注**: 针对自动驾驶、医疗等高风险领域，这是最可靠的方式。成本高昂，但不可或缺。
    *   **强模型蒸馏**: 使用 GPT-4V 或其他顶尖模型，根据我们的格式规范生成海量的指令-响应对。需要设计精巧的 prompt 并进行严格的质量过滤。
    *   **开源数据集重格式化**: 将现有的学术数据集（如 COCO, VQA, Ego4D）转换成我们的统一对话格式。
    *   **样本挖掘**: 专门构造模型容易出错的场景，例如混淆指令、伦理困境、传感器异常等，并提供正确的“拒绝回答”或“请求澄清”的范例。

> **经验法则 (Rule-of-Thumb):**
> SFT 数据集的构建应遵循“分层多样性”原则。首先确保覆盖所有核心能力（L1层多样性：VQA, ASR, Action Prediction...），然后针对每个能力，穷举不同的场景和提问方式（L2层多样性：雨天/夜间的驾驶决策，带有口音的语音指令...），最后在表达层面进行改写和扩充（L3层多样性）。SFT 阶段投入 100k-1M 条高质量、多样化的样本是 10B 级别模型的常见配置。

---

## 21.2 语音交互对齐：优化实时体验

语音交互的对齐核心在于“流动性”，即模仿人类对话的自然节奏和实时反馈。

1.  **延迟与流式生成 (Streaming)**:
    *   **挑战**: 用户说完话后，模型需要时间进行 `ASR -> NLU -> Policy -> NLG -> TTS` 的完整流程，导恼人的延迟。
    *   **对齐策略**: 训练模型具备流式生成的能力。即模型在完全想好整个句子之前，就能开始生成开头的词语。这需要在训练时模拟这种“边想边说”的过程，例如，将完整的 CoT 和响应序列，切分成多个小块进行预测。同时，可以引入一个与响应启动时间（Time-to-First-Token）相关的惩罚项。

2.  **打断与轮次管理 (Barge-in & Turn-taking)**:
    *   **挑战**: 对话不是独白。模型需要知道何时倾听，何时说话。
    *   **对齐策略**: 在训练数据中显式地建模这一过程。将实时的 VAD (Voice Activity Detection) 信号作为一个特殊的输入 token `<|vad_on|> / <|vad_off|>`。当模型正在生成响应时，如果输入流中出现 `<|vad_on|>`，则正确的目标输出应立即切换为 `<|interrupt_and_listen|>` 这样的特殊 token。

3.  **韵律与情感 (Prosody & Emotion)**:
    *   **挑战**: 机械的语调会破坏用户体验。
    *   **对齐策略**: 这超越了纯文本。一种方法是引入韵律控制 token，如 `<|pitch=high|> <|rate=fast|>`，并在训练时由声学模型提供监督信号。另一种更先进的方法是，让模型直接预测离散的声学 token（来自神经编解码器，如 EnCodec），从而实现文本和韵律的一体化生成。偏好数据中应包含对“声音是否自然/富有情感”的评价。

---

## 21.3 行动对齐：从模仿到决策

行动对齐的目标是让模型输出安全、有效、平滑的动作序列。

1.  **行为克隆 (Behavioral Cloning, BC) 及其局限**:
    *   **核心**: 将专家（人类驾驶员）的 `(Observation, Action)` 数据对作为监督信号进行模仿学习。这是最直接的起点。
    *   **陷阱**:
        *   **因果混淆 (Causal Confusion)**: 模型可能学到虚假的相关性。例如，模型发现每次踩刹车时，前方的刹车灯都会亮起，于是错误地认为“点亮自己的刹车灯”是导致前车减速原因。
        *   **协变量漂移 (Covariate Shift)**: 由于模型的不完美，它可能会进入一个与训练数据分布略有不同的状态（e.g., 车辆稍微偏离车道中心）。由于训练数据中没有覆盖这种“次优状态”下如何纠正的样本，模型可能会束手无策，导致误差累积，最终完全偏离。

2.  **超越简单 BC**:
    *   **DAgger (Dataset Aggregation)**: 一种经典的解决协变量漂移的方法。让模型在模拟器中运行，当它犯错时，由人类专家接管并提供正确的操作。将这些 `(模型犯错的Observation, 专家纠正的Action)` 数据点加入到训练集中，进行下一轮迭代。
    *   **目标条件 BC (Goal-Conditioned BC)**: 不仅模仿动作，还让模型理解动作背后的“意图”。输入变为 `(Observation, Goal)`，输出为 `Action`。例如，`Goal` 可以是“在下一个路口左转”。这大大增强了模型的泛化能力。
    *   **扩散策略 (Diffusion Policies)**: 动作序列的生成建模为一个从噪声到动作的去噪过程。这种方法在处理高维、连续的动作空间时表现出色，能够生成更平滑、更多样的轨迹。

3.  **离线强化学习 (Offline Reinforcement Learning)**:
    *   **动机**: BC 只能模仿“看到过的”行为，无法超越专家。RL 有潜力发现比专家更优的策略。但在线 RL（在真实环境中试错）对于自动驾驶来说过于危险和昂贵。
    *   **核心思想**: 从一个固定的、离线的专家数据集中学习。关键挑战是如何在不与环境交互的情况下，安全地评估和优化策略，避免对数据集中未见过的 `(state, action)` 对进行过高的价值估计。
    *   **代表算法**:
        *   **CQL (Conservative Q-Learning)**: 在标准的 Q-Learning 损失上增加一个正则项，惩罚在数据集分布之外的 `(state, action)` 对的 Q 值，同时推动数据集中存在的 `(state, action)` 对的 Q 值。这使得学习过程更加保守”和稳定。
        *   **IQL (Implicit Q-Learning)**: 避免对未见过的动作进行显式查询，而是通过分位数回归等技术隐式地学习 Q 函数，表现稳健。

> **经验法则 (Rule-of-Thumb):**
> 行动对齐的路径应为：**高质量 BC 基线 → DAgger 或 Goal-Conditioned BC 提升鲁棒性 → 谨慎引入 Offline RL 探索超越模仿的可能性**。在 1B 模型上跑通 BC 和 DAgger 流程，在 10B 模型上再尝试引入 Offline RL。

---

## 21.4 轻量偏好优化与安全约束

SFT 教会模型“能做什么”，偏好优化教会模型“应该做什么”。

1.  **DPO (Direct Preference Optimization) 深度解析**:
    *   **背景**: 传统的 RLHF 需要训练一个独立的奖励模型，再用 PPO 等强化学习算法去优化，流程复杂且不稳定。DPO 将这两个步骤合二为一。
    *   **数学原理**: DPO 的巧妙之处在于，它发现奖励模型和最优策略之间存在一个解析关系。因此，可以直接通过一个简单的分类损失来优化策略，而无需显式拟合奖励。
        $ \mathcal{L}_{DPO}(\pi_{\theta}; \pi_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma(\beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_{\text{ref}}(y_l|x)}) \right] $
        *   **$(x, y_w, y_l)$**: 一个偏好数据样本，包含指令 $x$、被偏好的响应 $y_w$ (chosen) 和被拒绝的响应 $y_l$ (rejected)。
        *   **$\pi_{\theta}$**: 我们要优化的模型。
        *   **$\pi_{\text{ref}}$**: SFT 阶段结束后的模型快照，作为参考。它的作用是防止 $\pi_{\theta}$ 为了迎合偏好而偏离原始预训练的数据分布太远，从而导致能力退化（灾难性遗忘）。
        *   **$\beta$**: 温度参数，控制我们对参考策略的偏离程度。$\beta$ 越高，模型越倾向于匹配偏好数据，但也可能牺牲多样性。

2.  **多模态偏好数据**:
    *   **语音**: `(指令, 更自然的语音响应, 更生硬的语音响应)`
    *   **驾驶**: `(路况视频, 更平稳安全的轨迹, 更激进危险的轨迹)`
    *   **VLA**: `(场景+指令, 准确执行并提供解释的视频, 执行错误或无响应的视频)`

3.  **安全对齐与 Constitutional AI**:
    *   **挑战**: 仅仅依赖人类标注来覆盖所有安全场景是不现实的。
    *   **Constitutional AI**: 这是一种扩展偏好学习的强大框架。
        1.  **定义原则 (Constitution)**: 制定一套指导模型行为的基本原则，例如“不提供有害建议”、“尊重用户隐私”、“驾驶时遵守交通法规”等。
        2.  **自我批判与修正**: 让 SFT 模型对一个有害指令生成一个初始响应。然后，让模型根据“原则”来批判这个响应，并重写一个更安全、更合规的响应。
        3.  **生成偏好数据**: 初始的有害响应成为 $y_l$，修正后的安全响应成为 $y_w$。这样可以大规模地、自化地生成用于安全对齐的偏好数据。

---

## 21.5 本章小结

本章系统地阐述了将一个强大的预训练模型转化为一个可靠、安全、有用的多模态智能体的对齐全过程。
-   我们始于 **SFT**，通过设计精巧的统一对话格式和构建分层多样的数据集，为模型注入了遵循指令和结构化思考的能力。
-   接着，我们针对 **语音交互** 和 **具身行动** 这两个高度动态的场景，探讨了特定的对齐策略，从流式生成到离线强化学习，解决了实时性和安全性的核心痛点。
-   最后，我们深入研究了以 **DPO** 为代表的偏好优化技术，它能让模型学习人类细致入微的偏好，并通过 **Constitutional AI** 等方法系统性地增强模型的安全性。
-   对齐是一个精细的雕琢过程，它决定了模型的最终形态和用户体验，其重要性不亚于预训练本身。

---

## 21.6 常见陷阱与错误 (Gotchas)

1.  **灾难性遗忘 (Catastrophic Forgetting)**
    *   **问题**: 在 SFT/DPO 阶段，模型过度拟合对齐数据，导致其在预训练阶段学到的通用知识（如事实性问答、代码能力）显著衰退。这种能力的下降被称为“对齐税”（Alignment Tax）。
    *   **调试与修复**:
        *   **数据混合**: 在 SFT 数据中混入 5-10% 的高质量预训练数据。
        *   **使用 `π_ref`**: DPO 中的参考模型 `π_ref` 本身就是一种防止过度偏离的正则化。
        *   **LoRA 等 PEFT 方法**: 只微调模型的一小部分参数，可以有效保留大部分预训练知识。

2.  **奖励“黑客” (Reward Hacking)**
    *   **问题**: 在偏好学习中，模型找到了一个“捷径”来获得高偏好分数，但实际上并没有真正理解或满足用户的意图。例如，模型发现冗长、礼貌的回答更容易被标注为“偏好”，于是开始生成大量无关紧要的客套话，而忽略了核心问题。
    *   **调试与修复**:
        *   **多样化标注团队**: 确保标注员背景多样，避免单一偏好主导。
        *   **迭代式红队测试 (Red-Teaming)**: 专门找人或用另一个 AI 来攻击模型，寻找奖励“黑客”的漏洞，并将这些案例加入到偏好数据集中。
        *   **更精细的奖励设计**: 在奖励模型训练（如果是RLHF）或偏好标注指南中，明确指出简洁性、信息量等也是重要的评估维度。

3.  **SFT/偏好数据中的偏见放大**
    *   **问题**: 标注员的社会、文化偏见会不知不觉地滲透到对齐数据中。模型在学习这些数据后，不仅会复现这些偏见，甚至会将其放大。
    *   **调试与修复**:
        *   **制定详尽的标注指南**: 明确要求标注员保持中立、客观，并提供处理敏感话题的具体指导。
        *   **偏见审计**: 使用专门的基准测试集（如 BOLD, Bias in Open-domain Language Generation）来评估模型的偏见水平。
        *   **对标注员进行无意识偏见培训**。

4.  **行动空间离散化误差 (Action Discretization Error)**
    *   **问题**: 在自动驾驶中，我们将连续的控制信号（如方向盘转角 `[-270, 270]` 度）离散化为有限的 token（如 `-5`, `-2`, `0`, `2`, `5` 度）。这可能导致模型无法做出精细的微调，或者在两个离散点之间震荡，产生不平顺的驾驶行为。
    *   **调试与修复**:
        *   **增加离散化的粒度**: 更细的 action bin，但会增加词表大小和预测难度。
        *   **混合方法**: 预测一个粗粒度的 action bin，并同时回归一个在该 bin 内的残差值（residual value）。
        *   **评估平顺性指标**: 在评测中加入如加加速度（Jerk）等指标，来量化轨迹的平顺性。

