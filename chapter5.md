# 第五章 数据采集 I：网页文本与代码（合规）

## [返回目录](index.md)

---

### **开篇段落**

如果说多模态大模型是通往通用人工智能的方舟，那么文本与代码就是这艘方舟的龙骨与神经网络。它们承载着人类积累的绝大部分知识、逻辑、文化与创造力，是模型建立世界模型（World Model）、理解复杂指令、进行高级推理的根本。本章将开启我们宏伟数据工程的第一步，深入探讨如何从浩瀚如烟的互联网中，以一种**合规、高效、可扩展**的方式，系统性地获取海量高质量的网页文本与源代码。我们将从顶层战略的“抓什么”（信源选择），到战术执行的“怎么抓”（框架与策略），再质量守门员的“如何筛”（初步清洗），覆盖全链路的核心实践。学习本章后，您将掌握为万亿（Trillion）级 token 规模的大模型项目奠定坚实、合规、高质量文本基座的全套方法论，并深刻理解在工业级规模下，平衡**数据质量（Quality）、采集数量（Quantity）、法律合规（Compliance）与工程成本（Cost）**这四大支柱的艺术。这项工作是 **[W3]** 里程碑——“数据配方（Data Recipe）初版冻结”——得以实现的技术前提。

---

### **文字论述**

#### 5.1 目标域与白名单源：从“淘金”而非“滤沙”开始

在启动一个涉及数千万亿 token 的项目时，最危险的误区是认为“数据越多越好”，并采取“先污染后治理”的策略。全网无差别抓取（“喝干消防栓里的水”）会引入海量的噪声、偏见、毒性内容和法律风险，其后期治理成本将呈指数级增长，甚至可能拖垮整个项目。因此，们的核心数据哲学是：**从源头开始，主动选择高质量信源（白名单策略），而非被动地从垃圾中过滤黄金。**

*   **为什么要坚定不移地执行白名单策略？**
    *   **信噪比的经济学**：高质量信源（如 arXiv, Stack Overflow, Wikipedia）的信噪比极高。与其花费 100 个 GPU 小时去清洗 1TB 的低质数据，不如花费 1 个 GPU 小时处理 100GB 的高质数据。在我们的规模下，这种成本差异会被放大百万倍。
    *   **风险控制**：白名单机制使得法务和合规团队可以对数据源进行前置审计，从根本上降低了版权侵权、违反服务条款（ToS）和隐私泄露的风险。
    *   **能力对齐**：通过精选信源，我们可以主动塑造模型的“知识结构”和“价值观”。例如，想让模型擅长科学推理，就加大对学术论文、教科书和科学百科的权重；想让它精通代码，就聚焦于顶级开源项目和官方开发者文档。

*   **构建与维护一个动态的“分级白名单”**
    一个有效的白名单不是一份静态列表，而是一个需要持续维护和迭代的动态系统。我们建议将其分为不同层级：
    *   **Tier 1 (黄金信源)**: 顶级的、几乎无需清洗的信源。例如：维基百科、arXiv、PubMed、古登堡计划（Project Gutenberg）、各国政府与国际组织官网（.gov, .edu, .org）、主流编程语言的官方文档、Stack Exchange 核心站点。这些是数据配方中的“基础营养”。
    *   **Tier 2 (白银信源)**: 内容质量较高，但可能混杂少量广告或低质内容。例如：信誉良好的技术博客、知名媒体、高质量的行业论坛、GitHub 上星标数较高的开源项目。这些数据需要经过更严格的过滤流程。
    *   **Tier 3 (青铜信源)**: 覆盖面广，但质量参差不齐。例如：大型新闻聚合网站、广泛的社交媒体（需通过 API 合规获取）。这类信源主要用于获取更广的语言现象和常识知识，需要最强的清洗和去重策略，且采样比例应受严格控制。

> **经验法则 (Rule-of-thumb):**
> 启动项目时，80% 的精力应投入到构建和扩充 Tier 1 和 Tier 2 的信源列表。这个列表本身就是项目最有价值的资产之一。应成立一个专门的“数据策展（Data Curation）”小组，成员包括 AI Scientist、工程师和领域专家，负责持续评估和更新这份名单。

#### 5.2 官方 API / 开源语料与“别人已洗好的包”的接入策略

在自建抓取系统运转之前，聪明的团队会站在巨人的肩膀上。最大限度地利用现有的、结构化的、合规的数据通路是项目初期快速启动的关键。

*   **API 优先原则**：始终将使用官方 API 作为获取特定平台数据的首选。
    *   **理由**：这是平台方明确授权的数据使用方式，完全合规。数据通常是结构化的（如 JSON），省去了复杂的 HTML 解析步骤。并且，API 会明确告知速率限制，便于我们进行礼貌且可预测的访问。
    *   **行动项**：为 Reddit、Stack Exchange、Twitter（现 X）、Wikipedia 等主要平台开发健壮的 API 客户端，内置重试、限流、错误处理逻辑，并严格遵守其开发者协议。

*   **拥抱高质量开源语料**：社区已经为我们准备了丰盛的“数据半成品”。
    *   **主要来源**：RedPajama, SlimPajama, The Pile, C4, RefinedWeb 等项目已经完成了大规模的网页抓取、清洗和初步去重工作。
    *   **接入策略：“信任但验证” (Trust, but Verify)**
        1.  **全量下载与索引**：将这些数据集视为我们的“上游依赖”。
        2.  **强制二次清洗**：绝不直接使用。必须将它们送入我们自建的、更严格的过滤和去重流水线（详见 5.5 和 5.6 节）。因为它们的清洗标准、去重粒度、PII 定义可能与我们的项目要求不符。
        3.  **跨数据集去重**：这是关键的一步。必须在我们自己抓取的数据、所有接入的开源数据包之间进行全局去重，避免模型在不同来源的相同内容上重复训练。

*   **决策矩阵：数据获取路径对比**

| 获取路径 | 优点 | 缺点 | 核心行动项 |
| :--- | :--- | :--- | :--- |
| **官方 API** | 合规、结构化、高效、风险低 | 覆盖范围有限、有严格速率限制 | 开发专用适配器，管理 API 密钥 |
| **开源语料包** | 快速启动、节省大量前期抓取成本 | 质量标准不一、可能包含“数据毒药” | 建立“信任但验证”的二次清洗与全局去重管道 |
| **自建爬虫** | 覆盖面最广、可控性最强 | 工程复杂度高、法律风险最高、成本高昂 | 严格执行白名单、合规抓取与精细化限流 |

#### 5.3 抓取框架与高并发限流：构建尊重网络的“数据舰队”

对于白名单中无法通过 API 或开源包覆盖的信源，我们需要构建一个工业级的分布式抓系统。这个系统的核心设计理念是**效率**与**礼貌**并重。

*   **框架的模块化设计**
    一个可扩展的抓取系统通常由以下解耦的微服务组成，通过消息队列（如 Kafka/Pulsar）和分布式 KV 存储（如 Redis/etcd）进行通信：

    ```ascii
    +-------------+   [URLs]   +-----------+   [Fetch Tasks]   +----------------+
    | URL Manager | ---------> | Scheduler | ---------------> | Fetcher Fleet  |
    | (Frontier)  |            +-----------+                   | (Stateless)    |
    +-------------+                 ^                          +-------+--------+
          ^                         |                                  | [Raw HTML]
          | [New URLs]              | [Task Ack]                         v
          |                         |                          +-------+--------+
    +-------------+   [Parsed]  +-----------+                  |  Parser Fleet  |
    |  Storage    | <---------  |  Parser   | <----------------+  (Stateless)   |
    | (Object/DB) |             +-----------+                                  |
    +-------------+                                                            |
          | [Content Hash]                                                     |
          +--------------------------------------------------------------------+
                                      (反馈给 URL Manager 用于去重)
    ```
    *   **URL Manager (Frontier)**: 维护待抓取 URL 队列，处理 URL 去重、优先级排序。
    *   **Scheduler**: 核心调度器，根据各域名的限流策略，从 Frontier 取出 URL，生成抓取任务并推送到消息队列。
    *   **Fetcher Fleet**: 大量无状态的下载服务实例，从消息队列消费任务，执行 HTTP 请求，并处理重定向、超时和错误。
    *   **Parser Fleet**: 无状态的解析服务，从 Fetcher 获取的原始 HTML/XML 中提取正文、元数据和新的出站链接。
    *   **Storage**: 将解析后的干净文本和元据持久化到对象存储（如 S3/Ceph）或分布式文件系统。

*   **合规抓取的铁律 (The Ironclad Rules of Compliant Crawling)**
    1.  **`robots.txt` 是法律，不是建议**：每个 Fetcher 在请求任何域名的任何资源前，**必须**检查并严格遵守该域名的 `robots.txt` 规则。这个逻辑应该是框架级的，无法被单个任务绕过。
    2.  **透明的 `User-Agent`**：设置一个清晰、诚实的 `User-Agent`，例如 `MyProject-VLABot/1.0 (+http://www.myproject.ai/bot.html)`。这个 URL 应该指向一个页面，解释我们的项目、数据用途和联系方式。
    3.  **尊重 `Crawl-delay`**：如果 `robots.txt` 中指定了 `Crawl-delay`，则以此作为两次请求间的最小间隔。
    4.  **模拟人类行为**：适当添加随机抖动（Jitter），避免过于规律的机器访问模式。发送标准的 `Accept`, `Accept-Language`, `Accept-Encoding` 头信息。

*   **高并发与分布式限流 (High Concurrency & Distributed Rate Limiting)**
    当你有成千上万个 Fetcher 实例时，如何确保对同一个域名（例如 `example.com`）的请求总和不超过设定的 QPS（Queries Per Second）？
    *   **中心化限流**：使用像 Redis 这样的内存数据库来为每个域名维护一个令牌桶或漏桶。
    *   **工作流程**：Fetcher 在请求 `example.com` 之前，先向 Redis 请求一个 `example.com` 的“令牌”。如果 Redis 中有令牌，则请求成功，Fetcher 继续；如果没有，则请求失败，Fetcher 等待一段时间后重试。Scheduler 也可参与决策，仅在预估有令牌时才分发任务。
    *   **动态调整**：监控 HTTP 响应码。当某个域名频繁返回 `429 Too Many Requests` 或 `503 Service Unavailable` 时，自动降低该域名的 QPS 上限。

> **经验法则 (Rule-of-thumb):**
> 默认全局 QPS 上限设为一个非常保守的值，例如 1 QPS per domain。对于白名单中的合作伙伴或明确表示欢迎爬虫的网站，可以根据其服务能力适当调高。永远不要因为追求速度而牺牲合规性和对他人的尊重。

#### 5.4 元数据/版权/许可记录：为每一条数据建立“身份档案”

在生产级项目中，无法溯源的数据等同于不可用数据。详尽的元数据记录是数据治理、合规审计、偏见分析和可复现研究的生命线。

*   **必须记录的核心元数据字段**：
    *   `source_url`: 数据的原始、规范化后的 URL。
    *   `crawl_timestamp_utc`: 抓取时的 UTC 时间戳（ISO 8601 格式）。
    *   `http_status_code`: HTTP 响应码（例如 200, 301）。
    *   `content_type`: 从 `Content-Type` 响应头解析的 MIME 类型。
    *   `document_hash`: 文档原始内容的 SHA-256 哈希值，用于精确去重。
    *   `parser_version`: 使用的解析器及其版本，便于未来需要重新解析时追溯。
    *   `license_info`: 自动从页面（如 `rel="license"` 链接、页脚文本）或代码库（`LICENSE` 文件）中提取的许可信息。
    *   `source_domain`: 数据来源的顶级域名。

*   **存储格式**：JSON Lines 是一个极佳的选择，每一行是一个独立的 JSON 对象，既结构化又易于流式处理。

```json
{
  "text": "The quick brown fox jumps over the lazy dog.",
  "metadata": {
    "source_url": "https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog",
    "crawl_timestamp_utc": "2024-05-21T10:00:00Z",
    "document_hash": "a8c0e4c8... (sha256)",
    "parser_version": "trafilatura-1.8.0",
    "license_info": "CC BY-SA 4.0",
    "source_domain": "wikipedia.org"
  }
}
```

#### 5.5 增量抓取与重复去重：应对动态世界的“数据免疫系统”

互联网是动态变化的，而重复内容是其最大的信息熵。我们的数据管道必须能高效地更新内容并剔除冗余。

*   **多阶段去重流水线 (Multi-Stage Deduplication Pipeline)**

    这是一个从粗到细、计算成本递增的过滤过程：

    ```ascii
    [Raw Documents]
          |
    +-----v---------------------------+
    | Stage 1: URL & 精确哈希去重    |  (过滤 ~80% 重复)
    | - URL 规范化                    |
    | - SHA-256 内容哈希              |
    +-----------------+---------------+
                      |
    +-----------------v---------------+
    | Stage 2: 近似内容去重           |  (过滤 ~15% 重复)
    | - SimHash + 海明距离 < 3        |
    | - MinHash + LSH                 |
    +-----------------+---------------+
                      |
    +-----------------v---------------+
    | Stage 3: 语义/跨语言去重 (可选) |  (过滤 ~1% 重复)
    | - 使用多语言 embedding 模型     |
    +-----------------+---------------+
                      |
              [Unique Documents]
    ```

    *   **Stage 1: 精确去重**: 使用一个庞大的分布式 KV 存储（如 RocksDB, LevelDB）来存储见过的所有 URL 规范形式和内容哈希值。这是最快、成本最低的去重步骤。
    *   **Stage 2: 近似内容去重**:
        *   **SimHash**: 将文档转换为一个紧凑的 64 位或 128 位指纹。其精妙之处在于，内容相似的文档，其 SimHash 指纹的海明距离（Hamming Distance，即二进制位不同的数量）非常近。我们可以设置一个阈值（例如 `d <= 3`），将距离小于该阈值的文档视为重复。
        *   **MinHash + LSH (局部敏感哈希)**: 对于超大规模数据集，O(n²) 的两两比较是不可行的。MinHash 将文档（视为 n-grams 集合）压缩成一个签名，而 LSH 是一种哈希技术，能让相似的签名以高概率“碰撞”到同一个桶里。这样，我们只需要在同一个桶内进行比较，将复杂度从 O(n²) 降至接近 O(n)。

*   **增量抓取**:
    对于需要定期更新的信源（如新闻网站、技术博客），在 Fetcher 层面利用 HTTP 的 `ETag` 和 `Last-Modified` 头部。发送 `If-None-Match` 和 `If-Modified-Since` 请求，如果服务器返回 `304 Not Modified`，则可跳过下载和处理，极大节省带宽和计算资源。

#### 5.6 初步过滤流水线：数据的“安检口”

在数据被正式接纳并进入昂贵的 Tokenization 和训练阶段之前，必须通过一道快速、自动化的质量安检。

1.  **HTML 内容提取与清洗**：原始 HTML 充满了导航栏、广告、页脚、脚本等“样板（Boilerplate）”代码。使用 `trafilatura`, `Beautiful Soup` 等库，结合启发式规则（如文本密度、链接密度）来精准提取核心正文内容。
2.  **语言识别 (Language ID)**：使用 `fastText` 的紧凑型预训练模型，它可以极快地（每秒处理数千篇文档）为每篇文档打上语言标签。不符合我们语种配比（见第四章）的文档在此阶段被丢弃或分流。
3.  **低质内容过滤**：
    *   **启发式规则**: 过滤掉文本过短/过长、符号/数字比例异常、平均词长过短的文档。
    *   **“乱码”检测**: 计算文本的压缩比，无法有效压缩的文本（如随机字符串）通常是乱码。
4.  **毒性与安全粗筛**：使用基于关键词列表和正则表达式的快速过滤器，标记出明显的仇恨言论、色情、极端暴力等内容。这是一个高召回率（宁可错杀，不放过）的阶段，更精细的判断留给第十一章的小模型分类器。
5.  **PII (个人身份信息) 粗筛**：利用正则表达式库（如 Google's `re2`）大规模扫描常见的 PII 模式，如电子邮件、电话号码、IP 地址、信用卡号等。对此类文档进行标记、丢弃或送入专门的脱敏流程。

---

### **本章小结**

本章为构建一个万亿级 token 的文本与代码数据集奠定了坚实的工程与合规基础。我们强调了**以质量为核心，而非数量**的数据哲学，并将这一理念贯穿于数据采集的全过程。

*   **顶层战略**：我们确立了**分级白名单**制度，优先从高质量信源采集数据，并制定了**API > 开源包 > 自建爬虫**的务路径。
*   **工程实现**：我们设计了一个**模块化、可扩展的分布式抓取框架**，并深入探讨了其合规抓取和**分布式限流**的核心技术细节。
*   **数据治理**：我们强调了**详尽元数据记录（数据溯源）**的极端重要性，并提出了一个**多阶段、由粗到细的去重流水线**，以应对精确和近似重复。
*   **质量保障**：在数据入库前，必须通过一道包含**内容提取、语言识别、低质过滤和安全粗筛**的初步过滤关卡。

---

### **常见陷阱与错误 (Gotchas)**

1.  **`robots.txt` 的误读与缓存失效**: 仅仅在爬虫启动时读取一次 `robots.txt` 是不够的，它可能会更新。**解决方案**: 为 `robots.txt` 设置一个合理的缓存有效期（如 24 小时），定期重新抓取。并确保解析库能正确处理 `Allow`, `Disallow` 的优先级和通配符 `*`。

2.  **字符编码地狱 (Character Encoding Hell)**: 网页编码五花八门（UTF-8, GBK, ISO-8859-1等）。错误的解码会导致乱码，污染整个数据集。**解决方案**: 优先相信 HTTP 头的 `Content-Type` 中的 `charset`。如果缺失，则使用 `chardet` 等库进行启发式检测。所有文本在内部处理和存储时，必须统一为 UTF-8。

3.  **哈希冲突与指纹长度选择**: 在使用 SimHash/MinHash 时，指纹长度是一个关键参数。64 位的 SimHash 在百亿级文档规模下，出现随机哈希碰撞的概率会显著增加，可能导致不相关的文档被错误地判为重复。**解决方案**: 根据数据集的最终规模估算，选择合适的指纹长度。对于万亿 token 规模，建议 SimHash 至少使用 128 位，MinHash 的签名长度也应相应增加。

4.  **动态页面的无底洞 (The Bottomless Pit of Dynamic Pages)**: 现代网站大量使用 JavaScript 动态加载内容，简单的 HTTP GET 请求只能拿到一个空壳。同时，网站的日历、分页、筛选器可以生成无限多的 URL 变体，形成“爬虫陷阱”。**解决方案**:
    *   对于 JS 渲染，集成一个无头浏览器（如 Puppeteer/Playwright）池，但仅对少数高质量、必须渲染的白名单网站使用，因为它成本极高。
    *   通过 URL 模式识别和路径深度限制来避免爬虫陷阱。例如，限制 URL 的动态参数数量或总长度。

5.  **元数据模式漂移 (Metadata Schema Drift)**: 在长达数月的数据采集中，你可能会决定增加一个新的元数据字段（例如，`is_human_translated`）。如果处理不当，会导致数据集的元数据结构不一致，给后续使用带来麻烦。**解决方案**: 制定严格的 schema 版本管理制度。任何 schema 变更都需要记录，并提供脚本来为旧数据填充新字段的默认值，确保整个数据集的元数据结构一致。
