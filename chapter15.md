# Tokenizer 设计：文本/音频/视频/图像/3D

### 15.0 开篇段落

如果说模型架构是智能体的骨骼，海量数据是其赖以生存的血液，那么 Tokenizer 则是连接物理世界与抽象思维的**神经系统**。它的核心使命是将来自文本、音频、视觉、三维空间等形态各异、统计特性迥然不同的信息流，**无损且高效地**转化为一个统一的、离散化的“语言”——即 Token 序列。本章的目标是设计并论证一套能够支撑 Vision-Language-Action (VLA) 模型的、工业级的多模态 Tokenizer 方案。我们将深入探讨每种模态的离散化哲学与技术路径：从成熟的文本 BPE，到解耦内容与风格的神经音频编解码，再到处理自动驾驶场景下**6摄480p@12Hz同步视频流**的挑战，以及如何以“代码即思想”的理念处理程序化 3D 数据。最终，我们将构建一个融合了离散 Token ID 与连续特征投影的混合输入管道，它不仅是后续自回归 Transformer 训练的基石，更直接决定了模型感知、理解和生成能力的上限。

### 15.1 文本：Qwen Tokenizer + 多模态扩词表

文本是连接所有模态的语义中枢，一个高质量的文本 Tokenizer 是整个系统的锚点。我们选择一个在多语言、多领域（特别是代码）上经过大规模验证的 Tokenizer 作为基座，并通过“词表扩充”策略，为其注入理解和组织多模态信息流的“语法”。

**1. 基座选择：为什么是 Qwen Tokenizer？**

*   **大规模多语言词表**：Qwen 的 Tokenizer 拥有约 15W 的词表规模，远超 Llama 等模型的 32K。这使得它在处理中英文（我们 90% 的数据构成）时能达到更高的压缩率，即用更少的 Token 示相同的信息。这对在有限上下文窗口内处理长视频和代码至关重要。
*   **代码与数字优化**：其 BPE 算法对代码和数字进行了特殊优化，这对于 VLA 场景中的动作指令（常以 JSON 或代码片段形式出现）以及 3D 建模脚本的 Tokenize 非常友好。
*   **成熟生态与稳定性**：作为一个广泛应用的基座，其健壮性和社区支持可以减少我们项目中的不确定性。

**2. 扩展示略：定义多模态的“元语言”**

我们不重新训练 BPE，而是采用**冻结基座、添加特殊 Token**的策略。这既能完全继承基座模型的语言能力，又能以最小代价引入新功能。这些特殊 Token 构成了模型解析复杂多模态输入的“语法规则”。

**特殊 Token 分类与设计哲学**：

| 类别 | 示例 Token | 设计目的与应用 |
| :--- | :--- | :--- |
| **模态边界** | `<\|image_start\|>`, `<\|video_end\|>`, `<\|audio_start\|>` | **显式信令**：清晰地告知模不同模态数据的起止边界，是处理交错模态序列的基础。 |
| **结构/语义** | `<\|thought\|>`, `<\|observation\|>`, `<\|action\|>` | **思维链与 VLA 结构**：引导模型生成可解释的中间推理步骤（thought），并严格区分环境观察（observation）与模型决策（action）。 |
| **元数据标记** | `<\|cam_front_center\|>`, `<\|timestamp_delta:0.083s\|>` | **时空锚定**：为每个数据片段打上时空标签。这对于自动驾驶中理解多摄视角关系和时序依赖是**不可或缺**的。 |
| **辅助信息** | `<\|ipa_start\|>`, `<\|ipa_end\|>` | **增强表示**：为特定模态（如音频）引入辅助的、更底层的表示，帮助模型进行更细粒度的理解和生成。 |

**一个更复杂的 VLA 场景示例**：

```ascii
<|user|>
回放一下刚才左侧有车辆切入的片段，并用语音解释危险点。
<|assistant|>
<|thought|>
用户请求回放特定事件并进行语音解释。步骤：1. 定位事件时间戳2. 提取相关多摄视频片段。3. 生成解释文本。4. 将文本合成为语音。5. 输出视频和音频。
<|thought_end|>
<|observation|>
<|video_start|>
<|timestamp_1678886400.100|>
<|cam_left_front|> [VID_TOK_1]...[VID_TOK_256]
<|cam_left_rear|> [VID_TOK_1]...[VID_TOK_256]
...
<|timestamp_1678886400.183|>
<|cam_left_front|> [VID_TOK_257]...[VID_TOK_512]
...
<|video_end|>
<|audio_start|>
<|ipa_start|> ðɪs ɪz... <|ipa_end|>
[AUD_CODEC_TOK_1]...[AUD_CODEC_TOK_M]
<|audio_end|>
```

**Rule-of-thumb**：
*   **数量控制**：新增特殊 Token 总数建议控制在 512 以内。过多的特殊 Token 会稀疏化嵌入空间，并略微增加 Softmax 计算的负担。
*   **初始化策略**：在 `model.resize_token_embeddings()` 后，新增 Token 的 embedding 向量应使用原词表中语义相近的 Token embedding 或所有向量的均值进行初始化，而不是随机初始化，这有助于加速新 Token 的学习。
*   **避免冲突**：确保特殊 Token 的字符串表示在有训练数据中都不会自然出现，防止歧义。

### 15.2 音频：神经编解码离散化（Codec）+ IPA 辅助 Token 层

音频作为一种高频连续波形，其 Tokenization 的核心挑战在于**信息压缩**与**特征解耦**。我们采用一种双轨并行的策略，将音频流分解为“说什么”（内容）和“怎么说”（风格）两部分。

**1. 主干表示：神经音频编解码器 (Neural Audio Codec)**

*   **原理**：我们采用基于 VQ-VAE 架构的预训练音频编解码器，如 EnCodec 或 SoundStream。这类模型通过一个卷积编码器将一小段音频波形（例如，约 20ms）压缩成一个低维度的隐向量，然后通过向量量化（Vector Quantization）模块，找到码本（Codebook）中与该隐向量最接近的码元（Code），输出其离散的索引 ID。这个过程类似于为“声音的笔画”建立了一本字典。
*   **信息瓶颈**：这个“编码-量化-解码”的过程构成了一个**信息瓶颈 (Information Bottleneck)**，迫使模型学习到音频中最具代表性的声学特征，而忽略掉冗余的噪声。
*   **多码本结构**：先进的 Codec 通常采用残差量化（Residual VQ），即使用多个码本。第一个码本捕捉主要信息，后续码本依次对前者的量化残差进行编码。这允许在码率（每秒生成的 Token 数）和重建质量之间灵活权衡。

$$
\text{Audio Clip} \xrightarrow{\text{Encoder}} \mathbf{z} \xrightarrow{\text{RVQ}} (\mathbf{c}_1, \mathbf{c}_2, ..., \mathbf{c}_K) \rightarrow (\text{ID}_1, \text{ID}_2, ..., \text{ID}_K)
$$

**2. 辅助表示：国际音标 (IPA)**

*   **价值**：IPA 是语音学的黄金标准，它提供了一种**跨语言、跨口音**的音素级别表示。对于我们的模型，引入 IPA 有三大好处：
    1.  **鲁棒性**：处理方言、少数语种和外语口音时，即使 ASR 转写的文本有误，正确的 IPA 序列也能为模型提供稳定的发音基础。
    2.  **可控性**：在成任务（TTS）中，可以直接编辑 IPA 序列来精确控制发音，实现“指哪读哪”。
    3.  **数据增强**：对于只有文本没有音频的数据，我们可以用 TTS 系统生成音频及其对应的 IPA，创造出高质量的（文本，IPA，音频 Codec）三元组数据。

**输入格式的协同工作**：

```ascii
                  ┌──────────────────┐
                  │ Raw Audio Wave   │
                  └────────┬─────────┘
                           │
           ┌───────────────┴───────────────┐
           ▼                               ▼
┌──────────────────┐             ┌──────────────────┐
│  ASR + IPA Tagger│             │ Neural Audio Codec │
└──────────────────┘             └─────────────────┘
           │                               │
           ▼                               ▼
┌──────────────────┐             ┌──────────────────┐
│ IPA Tokens       │             │ Codec Tokens     │
│ e.g., <ipa>hɛloʊ<ipa>│             │ e.g., [10,73,4]... │
└────────┬─────────┘             └────────┬─────────┘
           │                               │
           └───────────────┬───────────────┘
                           ▼
                 ┌──────────────────┐
                 │ Model Input      │
                 └──────────────────┘
```

**Rule-of-thumb**：
*   **码率预算**：假设使用 EnCodec，目标宽 6kbps，对应 75Hz 帧率和 8 个码本。一分钟的音频将产生 `75 * 8 * 60 = 36,000` 个 Token。这必须计入序列长度预算。对于非关键的背景音，可以降低码率（减少码本数）来节省长度。
*   **对齐方案**：IPA Token 序列通常比 Codec Token 序列短得多。在打包输入时，可以通过简单的重复或使用特殊的位置编码来将它们在时间上对齐。

### 15.3 图像/视频：VQ-GAN/VAE、时空标记与数据流预算

视觉信息是自动驾驶和具身智能的核心。其 Tokenization 面临着**维度诅咒**（高分辨率、多摄像头、高帧率）和**时空一致性**两大挑战。

**1. 离散化 vs. 连续特征：一个关键的架构抉择**

*   **方案 A：连续特征 + 投影器 (Projector)**：使用一个强大的预训练视觉编码器（如 ViT-22B, EVA-CLIP）提取图像/视频 Patch 的特征向量，然后通过一个小型 MLP 投影到 LLM 的词嵌入空间。
    *   **优点**：实现简单，能充分利用现有SOTA视觉模型的强大能力，推理速度快。
    *   **缺点**：对于 LLM 来说，这些投影后的 embedding 是“不透明”的黑箱。模型很难对其进行细粒度的推理、编辑或自回归地“逐像素”生成。这与我们的“生成-理解一体”目标相悖。
*   **方案 B：视觉码本 (Visual Codebook)**：训练一个类似 VQ-VAE 或 VQ-GAN 的模型，它能将图像 Patch 编码为离散的 Token ID。
    *   **优点**：将图像“翻译”成了 LLM 的母语——离散 Token。这使得模型可以像处理文本一样处理图像，实现 in-context 学习、编辑和高质量的自回归生成。
    *   **缺点**：需要额外训练一个高质量的视觉 Tokenizer，且重建质量和语义保真度是关键挑战。

**我们的选择**：**以方案 B 为主，方案 A 为辅**。主体视觉信息流采用 VQ-GAN 离散化，以赋能生成能力。而一些动态的、高精度的元数据（如精确的相机外参、IMU 读数）则通过投影器作为连续特征注入。

**2. 应对 6-camera 480p@12Hz 的数据洪流**

让我们量化挑战：
*   单帧分辨率：`640x480` (标准 4:3 的 480p)
*   Patch 大小：`16x16`
*   单帧 Patch 数：`(640/16) * (480/16) = 40 * 30 = 1200`
*   单时间步（所有摄像头）Patch 数：`1200 * 6 = 7200`
*   每秒 Patch 数：`7200 * 12 Hz = 86,400`

直接将这些 Patch 输入 LLM 是**绝对不可行**的。解决方案是引入一个强大的**时空视觉编码器**作为预处理器：
1.  **空间压缩**：该编码器首先对每个摄像头的 1200 个 Patch 进行编码，输出更少的、但信息更密集的特征向量（例如，通过 Attention Pooling 或 Resampling，降到 256 个向量）。
2.  **时间压缩**：可以进一步对时间维度进行压缩，例如，将 12Hz 的帧输入编码器，但只在 3Hz 的频率上输出 Token。
3.  **离散化**：最后，将这些压缩后的时空特征向量送入 VQ 模块，得到最终的视觉 Token 序列。

**Rule-of-thumb**：
*   **视觉 Token 预算**：经过上述压缩，一个合理的目标是将每秒的 86,400 个原始 Patch 转化为 1000-2000 个视觉 Token。这个数量级才能在数万的上下文窗口中表示几秒到十几秒的驾驶场景。
*   **相机位姿嵌入**：不要将相机内外参文本化。更好的方法是将其转换为一个紧凑的向量（例如，用一个小型 MLP 处理四元数和平移向量），然后作为一个特殊的**连续特征**嵌入到 Token 序列的开头，为该摄像头的视觉 Token 提供空间上下文。

### 15.4 3D：程序化优先，结构化次之，网格回落

3D 数据的 Tokenization 哲学是：**尽可能在高的抽象层次上进行操作，最大化利用语言模型的代码和逻辑推理能力**。

**1. 最高优先级：程序化几何/脚本 (Blender/CAD DSL)**
*   **理念**：一个 3D 模型不仅仅是点和面的集合，更是创造它的**意图和过程**的体现。将其表示为生成它的码（例如 Blender Python 脚本），是从根本上拥抱了“生成模型”的思想。
*   **Tokenization**：直接应用我们的 Qwen 文本 Tokenizer。模型学习的不是像素或顶点，而是 `create_cube`, `extrude`, `apply_material` 等构成 3D 世界的“动词”和“名词”。
*   **优势**：极高的信息压缩率、天然的可编辑性、以及与人类设计师思维过程的一致性。

**2. 次优先级：X3D 等结构化文本格式**
*   **理念**：当无法获得生成脚本时，退而求其次选择能够描述场景结构和语义的格式。X3D/VRML 就像 3D 世界的 HTML，它用层级化的节点和属性来定义场景。
*   **Tokenization**：同样是文本 Tokenizer。模型学习解析和生成这种具有清晰语法的结构化文本。

**3. 回落方案：.obj 等传统网格格式**
*   **挑战**：.obj 是一种“扁平”的格式，只包含顶点坐标和面索引，丢失了所有高层语义。直接处理浮点数对 LLM 也不友好。
*   **Tokenization 策略**：
    1.  **量化**：将所有顶点坐标归一化到 `[-1, 1]` 区间，然后进行量化，例如量化到 1000 个 bin。每个 bin 对应一个专门的数字 Token。
    2.  **序列化**：将 .obj 文件转换为一种特殊的语言格式：
        `<|3d_obj_start|> <|v|> [TOK_512] [TOK_101] [TOK_833] <|v|> ... <|f|> [IDX_1] [IDX_2] [IDX_3] <|f|> ... <|3d_obj_end|>`
        其中，`[TOK_X]` 是量化后的坐标 Token，`[IDX_Y]` 是顶点索引 Token。
*   **代价**：序列长度极长，且模型需要从零开始学习基础的几何概念，效率远低于前两种方案。

**Rule-of-thumb**：
*   在数据收集中，应不惜代价优先获取或逆向工程出 3D 模型的程序化表示。一个 1MB 的 .obj 文件可能需要数万个 Token 来表示，而生成它的 Python 脚本可能只需要几百个 Token。

### 15.5 连续特征兼容：作为“外挂”的通用投影器

尽管我们的核心是离散 Token，但系统必须有一个优的机制来接入无法或不适合离散化的连续特征。**跨模态投影器 (Cross-modal Projector)** 就是这个通用接口。

*   **架构**：通常是一个轻量的 MLP 或几层 Transformer Encoder。它的输入是任意维度的外部特征向量（如 IMU 6轴读数、CAN 总线车速、相机焦距等），输出是符合 LLM 内部维度的 embedding 向量。
    $$ \mathbf{e}_{\text{proj}} = \text{LayerNorm}(\text{MLP}(\mathbf{v}_{\text{in}})) \quad \text{where} \quad \mathbf{v}_{\text{in}} \in \mathbb{R}^{d_{\text{in}}}, \mathbf{e}_{\text{proj}} \in \mathbb{R}^{d_{\text{model}}} $$
*   **混合输入序列**：模型的第一层 Transformer 看到的输入将是一个混合序列，其中既有通过 Token ID 从 embedding table 中查出的向量，也有通过投影器实时计算出的向量。

```ascii
                                            ┌────────────────────┐
                                            │ Special Continuous │
                                            │ Feature Tokens     │
                                            └────────┬───────────┘
                                                     │ Projector
                                                     ▼
┌────────────┐   Embedding   ┌────────────────┐  ┌────────────────┐
│ Token IDs  ├─────────────► │ Token          ├─┐│ Fused          │
│(Text, VQ, │     Table     │ Embeddings     │ └►│ Input Sequence │
│ Codec...)  │               └────────────────┘   │                ├─► Transformer
└────────────┘                                     └────────────────┘
```

### 15.6 最终词表与对齐设计

我们的多模态词汇系统是一精心设计的复合体：

| 类别 | 来源/技术 | 规模估算 | 作用 |
| :--- | :--- | :--- | :--- |
| **基础文本** | Qwen BPE | ~150,000 | 提供强大的核心语言能力（中/英/代码）。 |
| **控制/元数据** | 手动添加 | ~512 | 作为多模态序列的“语法”和“标点”，组织信息流。 |
| **音素 (IPA)** | 标准 IPA Chart | ~200 | 提供与口音/语种无关的底层发音表示。 |
| **视觉码本** | VQ-GAN 训练 | 8,192 - 16,384 | 图像和视频的离散化“像素”词汇。 |
| **音频码本** | EnCodec (RVQ) | 8 codebooks * 1024/codebook = 8,192 | 音频波形的离散化声学基元词汇。 |
| **3D 数字量化**| 手动添加 | ~1,024 | 用于 .obj 方案中量化坐标值。 |
| **总计 (估算)** | | **~180,000** | 一个庞大但结构清晰的多模态统一词表。 |

这些设计共同确保了来自不同源头的信息流能够在一个统一的框架下被对齐、理解和生成，为构建一个真正的通用 VLA 模型奠定坚实的基础。

### 本章小结

本章，我们从第一性原理出发，为 VLA 多模态大模型设计了一套全面而深入的 Tokenizer 方案。我们确立了**万物序列化**的核心思想，并为每种模态制定了兼顾信息保真度、压缩效率和模型亲和度的策略。

*   **统一与分治**：通过扩充特殊 Token 的文本 Tokenizer 实现了对多模态流的统一调度，同时为各模态设计了专门的离散化方案（神经 Codec、VQ-GAN、程序化表示）。
*   **内容与风格解耦**：在音频处理中引入 IPA，实现了内容（音素）与风格（音色、韵律）的分离，增强了模型的鲁棒性和生成的可控性。
*   **抽象层次优先**：在 3D 处理中，我们建立了“程序化 > 结构化 > 网格化”的明确优先级，旨在最大化利用 LLM 的逻辑推理能力。
*   **预算与现实**：我们量化了自动驾驶视频流带来的数据挑战，并提出了基于时空编码器的压缩策略来管理列长度预算。
*   **开放与兼容**：通过通用投影器机制，为系统保留了接入任意连续特征的“即插即用”能力，确保了未来的可扩展性。

这套 Tokenizer 不仅仅是一个预处理工具，它本身就是模型世界观的一部分，是我们将物理世界的复杂性映射到模型可理解的符号空间的第一步，也是最关键的一步。

### 常见陷阱与错误 (Gotchas)

1.  **Tokenizer 版本不一致 (The Silent Killer)**：在分布式训练、离线数据处理和在线推理等多个环节中，使用了哪怕有细微差异的 Tokenizer 版本（例如，特殊 Token 的 ID 顺序不同），都会导致模型行为异常且极难调试。**解决方案**：将 Tokenizer 的配置文件（`tokenizer.model`, `special_tokens_map.json` 等）作为核心资产进行严格的版本控制（如 Git LFS），并在每次任务启动时强制校验其哈希值。

2.  **视觉 Token 质量差导致“视觉退化”**：如果用于离散化的 VQ-GAN/VAE 模型训练不佳，重建的图像模糊或失真，那么 LLM 接收到的就是“损坏”的视觉信息。它可能会学会忽略这些不可靠的视觉 Token，退化为一个纯文本模型。**解决方案**：在主模型训练前，对视觉 Tokenizer 进行独立的、严格的评测，确保其重建质量（PSNR, SSIM）和感知质量（LPIPS）达到高标准。

3.  **上下文长度被低价值信息占满**：在自动驾驶场景中，大部分视频帧可能是重复的（如等红灯）。如果不对视频流进行智能采样或压缩，这些冗余信息会迅速占满宝贵的上下文窗口，挤掉关键的瞬时事件信息。**解决方案**：在数据预处理阶段引入场景变化检测或关键帧提取算法，对静态或低信息量的片段进行降采样，动态调整 Tokenization 的频率。

4.  **模态边界的“渗透”效应 (Bleeding Edge)**：如果模型在训练时没能很好地学习模态边界 Token（如 `<|video_start|>`），在生时可能会出现“模态混淆”，例如在文本描述中突然插入了几个视觉 Token。**解决方案**：在数据构建时，确保边界 Token 的使用严格且一致。在损失函数设计上，可以考虑对边界 Token 的预测施加更高的权重，或者设计专门的辅助任务来强化模型对数据流结构的理解。

5.  **时间戳与物理世界脱节**：错误地处理时间戳（例如，使用了相对时间戳但重置点不一致，或浮点数精度问题）会导致模型无法建立正确的时序因果链。对于自动驾驶，这可能是灾难性的。**解决方案**：使用统一的、高精度的绝对时间戳（如 Unix timestamp in microseconds）。在 Tokenization 时，可以将其量化为一个基准时间和若干个 delta 时间 Token，以在压缩率和精度之间取得平衡。
