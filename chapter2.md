# 第 2 章：全局时间线与里程碑（W0–W26）

## 2.1 开篇段落

本章是整个预训练项目的战略地图与行动指南，其核心是将一个看似不可能的宏大工程——在 26 周（约半年）内从零开始交付一个生产级的多模态大模型——转化为一套逻辑严密、风险可控、步步为营的执行框架。我们将超越简单的日期罗列，深入探讨每个阶段的**哲学、关键依赖、量化验收标准和团队焦点切换**。这不仅是一份时间表，更是协调数据、模型、基础设施、评测等多个专业团队，使其在同一个节拍上协同作战的“总指挥图”。学习本章，你将掌握如何将学术界的前沿探索，转化工业界可预测、可交付的系统工程。无论你是设计模型的 AI Scientist，还是保障算力的 Infra 工程师，这份时间线都将是你定位自身角色、预判下游需求、管理个人与团队预期的关键罗盘。

## 2.2 文字论述

一个成功的万亿 token 级预训练项目，与实验室里探索性的研究有着本质区别。它不是一次性的冲刺，而是一场需要精确补给、严密监控和强大后勤保障的持久战。我们的 26 周计划围绕“**分阶段解耦、快速验证、持续交付**”的核心思想展开，确保在投入巨额资源的主训练开始前，所有关键风险都已得到识别和缓解。

### 2.2.1 总览甘特图：多战线并行与关键路径分析

项目的复杂性在于数据处理、模型研发、基础设施搭建和训练本身是高度并行的。任何一条战线的延误都可能导致整个项目停滞。甘特图清晰地揭示了各工作流之间的依赖关系和关键路径。

```ascii
          | W0 ----- W6 ----- W10 ------ W18 ------ W22 ----- W26 |
          | [规划与基建] [1B验证] [--- 10B 主训练 ---] [评估与收尾] |
----------+-------------------------------------------------------+
数据管道  | ███████████████████████████████████████████████████ >
 (采集/治理)| MVP (W6)    |          持续增量与迭代                  |
          | CRITICAL PATH: 主训练的燃料，必须先行                   |
          |                                                       |
模型与架构  | █████████ > |                                        |
 (设计/分词)| 方案冻结(W2)| 后续为维护与微调，非关键路径             |
          |                                                       |
基础设施  | ████████████████ >                                     |
 (集群/IO) | PoC (W2)    | 10B适配(W8)| 持续运维，保障稳定             |
          | CRITICAL PATH: 训练的载体，必须先行                   |
          |                                                       |
训练与实验  |      [PoC]  | █████████ >| █████████████████████████ > |
 (1B/10B) |             | 1B E2E(W10)| 10B 主训练 (W11-W18)         |
          | CRITICAL PATH: 项目的核心产出环节                     |
          |                                                       |
评测与对齐  |             |            | █████████████████████████ >
 (Eval/RL)  |             | 启动准备(W10)| 中期评测(W19-22)          |
          |                                                       |
交付与文档  |             |            |                      ███████ >
 (包/复现)  |             |            |                    发布(W26)|
----------+-------------------------------------------------------+
```
*   **关键路径分析**: 明显地，**数据管道**和**基础设施**是项目早期的两条关键路径。1B 模型的端到端验证（W10）是一个重要的“汇合点”，在此之前，数据和基建必须就绪。随后，**10B 主训练**成为新的、也是最长的一条关键路径。
*   **Rule-of-thumb**: 数据准备工作必须**至少提前 4-6 周**于主训练启动。这个缓冲期不仅用于生产数据，更关键的是用于**稳定数据管道**。一个不稳定的数据管道（例如，频繁因上游 API 变更或脏数据格式而出错）在主训练期间是致命的。

### 2.2.2 关键里程碑详解 (Phase by Phase)

**第一阶段：规划、验证与数据 MVP (W0–W6) —— “地基与勘探”**

*   **团队焦点**: 架构师、高级工程师、法务。目标是做出正确的技术选型，并用最小代价验证其可行性。
*   **[里程碑 W0–W2] 方案冻结 & PoC (Proof of Concept)**
    *   **交付**:
        1.  **《技术方案设计文档》V1.0**: 这不是一份空泛的 PPT，而是包含具体技术规格的工程蓝图。至少应包括：
            *   **数据配比表 V0.1**: 明确 30T token 的初步分解，例如：15T Common Crawl (RefinedWeb-style), 5T 代码, 2T 书籍, 3T 图像-文本对, 3T 视频-文本对, 1T 音频-文本对, 1T 合成数据。
            *   **Tokenizer 规范**: Qwen Tokenizer 基础词表，以及多模态特殊 token（如 `<|image|>`, `<|video_start|>`, `<|camera_front|>`, `<|ipa_phoneme|>`）的设计与 ID 范围。
            *   **模型结构参数**: 1B 和 10B 模型的层数、头数、隐层维度、MoE 专家数、路由策略等。
            *   **并行策略**: 针对 256xH100 集群的 TP/PP/DP/EP 组合方案，以及通信拓扑映射。
            *   **FP8 实现方案**: 明确使用 TransformerEngine 的哪个版本，FP8-E4M3/E5M2 的选择，以及哪些模块（Attention/FFN）启用 FP8。
        2.  **PoC 成功报告**:
            *   **目的**: 回答一系列“Yes/No”问题：我们的数据加载器能否在 8 卡上喂饱 GPU？我们的自定义 MoE 路由代码是否能正确反向传播？FP8 在一个小的 Transformer 上是否能收敛？
            *   **内容**: 包含在小数据集（如 C4 的一个分片）和 8-16 个 GPU 上运行 1000 步的 loss 曲线、GPU 利用率截图、NVLink 通信带宽图。
    *   **验收标准**: 方案通过架构委员会评审，无重大分歧；PoC 脚本和环境可被任何团队成员一键复现，loss 曲线平滑下降，无 NaN。

*   **[里程碑 W3–W6] 数据采集与治理 MVP (Minimum Viable Product)**
    *   **交付物**:
        1.  **数据管道 V1**: 一套可工作的、自动化的脚本和服务集合。它应该能做到：从公开源（如 Common Crawl, GitHub, YouTube API）拉取数据 -> 执行多阶段清洗和过滤（见 Chapter 10-11） -> Tokenize -> 打包成 WebDataset/TFRecord 格式 -> 推送到对象存储。
        2.  **`dataset-v0.1` (约 1-2T tokens)**: 这是第一批“航空燃油”，是启动 1B 模型训练的基础。它必须包含所有目标模态，即使某些模态的数据量还很小。例如：1T 文本，200B 图像 token，100B 音频 token，200B 视频 token。
        3.  **数据治理仪表盘 V1**: 一个可交互的前端页面，展示：
            *   各数据源的 token 数量和占比。
            *   语言分布饼图（含方言/少数语种）。
            *   PII/毒性内容检出率随时间变化的趋势图。
            *   跨模态数据对齐的统计信息（如视频-字幕时间戳差异分布直方图）。
    *   **验收标准**: 数据管道连续无故障运行 72 小时以上；在 32 个节点上测试数据加载，平均带宽能满足 1B 模型训练 MFU > 40% 的要求；仪表盘数据刷新延迟低于 24 小时。

**第二阶段：1B 基线与 10B 冲刺准备 (W7–W10) —— “全面彩排”**

*   **团队焦点**: 模型工程师、系统工程师。从验证单点技术转向整合所有组件，进行满负荷压力测试。
*   **[里程碑 W7–W10] 1B 模型端到端跑通**
    *   **目标**: 这不是为了训练一个 SOTA 的 1B 模型，而是**为了给 10B 训练进行一次成本可控的全流程、全系统演练**。
    *   **交付物**:
        1.  **1B 预训练检查点**: 在 1-2T token 数据上训练得到的模型权重。
        2.  **1B 训练性能分析报告**: 这比模型本身更重要。内容包括：
            *   端到端吞吐量（TFLOPs）和 MFU（Model FLOPs Utilization）曲线。
            *   瓶颈分析：通过 Profiling 工具（如 Nsight Systems）确定瓶颈是数据 IO、CPU 预处理、还是节点间通信。
            *   故障恢复演练记录：手动 kill 掉几个节点，测试 checkpoint 加载和重启流程需要多长时间，是否会自动剔除故障节点。
        3.  **初步评测报告**: 在几个核心的学术 benchmark（如 GLUE dev, ImageNet validation, Librispeech test-clean）上运行，确保模型学到了跨模TA的基本关联性，loss 下降不是假象。
        4.  **10B 训练启动手册 V1.0**: 一份详尽的操作指南（SOP），包含最终的启动命令、配置文件、监控仪表盘链接、以及应急预案联系人。
    *   **验收标准**: 1B 模型 loss 稳定收敛，关键指标显著优于随机基线（例如，语言模型的 Perplexity 低于某个阈值）；训练期间集群有效利用率（排除故障和重启时间） > 95%；10B 启动手册通过所有相关工程师的交叉评审。

**第三阶段：10B MoE 模型主线预训练 (W11–W18) —— “深海航行”**

*   **团队焦点**: **SRE (站点可靠性工程师)**、值班工程师。研发工作暂停，团队的核心使命从“创造”转为“守护”。
*   **[里程碑 W11–W18] 10B 主线预训练 (目标 10T token)**
    *   **交付物**:
        1.  **版本化的 10B MoE 模型检查点**: 按固定的 token 间隔（如每 500B tokens）保存一个不可变、带标签的 checkpoint。例如 `checkpoint_1.5T_tokens`。
        2.  **实时作战室监控大屏**: 聚合了所有关键监控视图：
            *   **全局视图**: 总 loss 曲线、学习率、梯度范数、已处理 token 数。
            *   **硬件视图**: 集群所有 GPU 的温度、功耗、显存使用率的热力图。
            *   **网络视图**: NVLink/IB 带宽矩阵，及时发现通信拥堵的节点。
            *   **MoE 专家视图**: 专家路由的负载均衡系数（Coefficient of Variation）、每个专家的利用率直方图、Router z-loss。这是诊断 MoE 模型训练问题的生命线。
        3.  **7x24 小时值班日志与故障复盘报告 (Postmortem)**: 详细记录每一次训练中断的现象、根因分析、解决方案和预防措施。
    *   **验收标准**: 训练任务完成 10T token 的数据处理量；loss 曲线全程平滑下降，无无法恢复的发散或长时间平台期；平均 MFU 达或超过预设目标（例如，对于 H100 上的 FP8 训练，目标 > 50%）；发生的所有 SEV-1/2 级故障都在 4 小时内得到解决，并有完整的复盘文档。

**第四阶段：评估、精调与交付准备 (W19–W26) —— “收获与封装”**

*   **团队焦点**: 评测团队、产品团队、技术写作。将原始的 checkpoint 转化为可评估、可使用、可理解的最终产品。
*   **[里程碑 W19–W22] 蒸馏与综合评测**
    *   **交付物**:
        1.  （可选）**蒸馏模型检查点**: 如果项目目标包含一个更小的、用于部署的模型，则在此阶段完成。
        2.  **综合评测报告 V1.0**: 这是一份深度、全面的模型“体检报告”，包含：
            *   在数十个公开和内部 benchmark 上的得分，并与 SOTA 模型进行详细对比。
            *   **红队测试 (Red Teaming) 报告**: 专门测试模型的安全性、偏见和有害内容生成能力。
            *   **长上下文能力估**: 使用“大海捞针”等方法测试模型的有效上下文长度。
            *   **多模态能力质性分析**: 提供精选的 case study，展示模型在复杂 VLA、自动驾驶场景理解、多语种语音交互等任务上的惊艳和失败案例。
    *   **验收标准**: 评测流程完全自动化，结果可一键复现；评测报告不仅有数字，还有深入的洞察，能明确指出模型的当前边界和下一步的优化方向。

*   **[里程碑 W23–W26] 收尾、复现包与发布**
    *   **交付物**:
        1.  **最终版模型检查点 (`final-checkpoint-v1.0`)**: 经过最终验证和打包的权重文件。
        2.  **模型卡 (Model Card)**: 遵循业界最佳实践（如 Google 或 Hugging Face 的模板），详细说明模型的设计、训练数据（及合规声明）、预期用途、限制、偏见分析和环境影响（碳足迹估算）。
        3.  **技术报告/博客**: 面向公众或学术界，阐述项目的技术创新和关键发现。
        4.  **可复现性工具包**: 一个包含了以下内容的 Git 仓库或容器镜像：
            *   加载模型权重并进行推理的示例代码。
            *   运行核心评测 benchmark 的脚本。
            *   一个 `Dockerfile` 或 `Singularity` 文件，固化了所有软件依赖。
            *   一个小型的示例数据集，用于快速验证端到端流程。
    *   **验收标准**: 一个不了解本项目的工程师，能够依据该工具包，在 1 小时内成功在单张 GPU 上运行模型推理，并在 24 小时内复现至少一个核心评测指标的结果。

### 2.2.3 风险缓释检查点

项目全周期都需要主动管理风险。以下是几个必须严肃对待的检查点：

*   **数据合规风险 (W3 前)**: 法务团队必须完成对所有计划使用的数据源的合规性审查。**“先用后审”是绝对禁止的**。对有争议的数据源（如包含大量用户生成内容但许可模糊的网站），必须定清晰的“最小化使用”或“匿名化处理”策略，并获得书面批准。
*   **算力稳定性风险 (W8 前)**: 在 1B 训练和 10B 启动前，必须对 256 卡集群进行至少 48 小时的满载稳定性测试。这不仅仅是跑 Linpack，而是运行一个与真实训练负载相似的模拟任务（例如，用合成数据训练一个中等规模模型），以暴露硬件、网络、文件系统在长时间高压下的潜在问题，并提前识别和隔离“坏节点”。
*   **存储与IO带宽风险 (贯穿全程)**: 视频数据是 IO 吞噬兽。必须进行量化估算：`6 cams * 480p_resolution * 3 channels * 12 Hz = ~10 MB/s per sample`。若全局批尺寸为 1024，则需要 `~10 GB/s` 的持续读取带宽。必须提前测试对象存储的吞吐量能否满足此需求，并考虑使用基于 SSD 的高速缓存层（如 Alluxio）来缓解压力。
*   **模型收敛风险 (W12 前)**: 若 10B 模型在训练的第一个 1T token 内出现 loss 不收敛、频尖峰、或 MoE 专家负载严重失衡（例如，超过 50% 的 token 被路由到前 10% 的专家），必须立即暂停。成立一个由模型、系统、数据专家组成的快速响应小组，利用诊断工具分析梯度流、激活值分布和数据样本，找到根因。**“再跑跑看”的心态是大型训练项目中最昂贵的错误之一。**

## 2.3 本章小结

本章提供了一份为期 26 周的多模态大模型预训练项目作战地图。它将一个宏大目标解构为四个逻辑递进的阶段，强调了从**研究探索到工业生产**的范式转变：
1.  **地基与勘探 (W0-W6)**：通过严谨的方案设计、PoC 验证和数据 MVP，将不确定性前置，用最小成本锁定技术路线。
2.  **全面彩排 (W7-W10)**：利用 1B 模型进行全系统、全流程的压力测试，其核心产出是**成熟的流程和有经验的团队**，而非模型本身。
3.  **深海航行 (W11-W18)**：进入高强度的 SRE 模式，核心目标是**保障练过程的绝对稳定和高效**，将意外中断的损失降到最低。
4.  **收获与封装 (W19-W26)**：将训练出的原始权重，通过系统性的评测、对齐和文档化，转化为**可信、可用、可复现**的高价值资产。

遵循这条时间线，并对每个里程碑的量化验收标准进行严格把关，是将巨额的算力投资转化为预期模型能力的最可靠路径。

## 2.4 常见陷阱与错误 (Gotchas)

1.  **陷阱：数据管道的“完美主义” vs “技术债”**
    *   **表现**: 团队要么试图在 W6 前构建一个能处理所有异常、覆盖所有角落案例的“完美”数据管道，导致延期；要么为了赶进度，在管道中硬编码了大量临时补丁，导致后期维护成本激增。
    *   **调试技巧**: 采用“**版本化迭代**”策略。`dataset-v0.1` 的目标是“跑通”，允许存在已知的小问题。同时建立“数据质量问题跟踪”系统，将发现的问题（如某种格式的字解析失败）记录在案，规划在 `v0.2` 或 `v0.3` 中修复。这使得模型训练可以与数据质量提升并行。

2.  **陷阱：轻视 1B 模型“彩排”的财务意义**
    *   **表现**: 管理层或团队认为 1B 训练是“浪费算力”，不如将资源全部投入 10B 模型。
    *   **调试技巧**: 用财务数据说话。估算 10B 训练每小时的成本（例如 `256卡 * $4/卡/小时 = ~$1024/小时`）。一次因可预见问题（如数据加载 bug）导致的 12 小时中断，损失超过 1.2 万美元。而为期三周的 1B 训练总成本可能只相当于 10B 训练几天的成本。这是一笔极其划算的“系统性风险保险”。

3.  **陷阱：“沉没成本”谬误**
    *   **表现**: 10B 训练在 W13 时发现 loss 曲线开始缓慢偏离预期，但团队因已投入了数百万的计算资源，不愿意停下来彻底诊断，而是寄希望于“它可能会自己变好”。
    *   **调试技巧**: 建立**预设的“中止训”规则**。例如：“如果连续 24 小时 loss 上升，或连续 72 小时 loss 持平，则自动暂停训练并触发告警。” 领导层必须明确支持“果断止损”的文化，承认重新启动有时是更快到达终点的路径。

4.  **陷阱：评测的滞后性与“最后一公里”问题**
    *   **表现**: 团队直到 W18 训练结束后才开始搭建评测流水线，结果发现评测环境配置复杂、数据下载缓慢、脚本有 bug，导致模型能力的反馈周期长达数周，严重影响了后续的对齐和优化工作。
    *   **调试技巧**: **评测流水线的开发应与主训练并行**。在 W10 产出 1B 模型后，评测团队就应该用它来测试和完善整个评测框架。目标是在 10B 训练期间，就能做到对中间 checkpoint 进行“夜间自动评测”，在第二天早上生成报告。
