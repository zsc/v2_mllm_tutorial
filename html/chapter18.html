<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>[chapter19.md] 训练配方：从 1B 到 10B 的生产级方案</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">面向 Vision‑Language‑Action（VLA）、自动驾驶/具身与语音交互的多模态大模型预训练教程（公开版）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter1.md] 前言、范围与读者指南</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 2 章：全局时间线与里程碑（W0–W26）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第三章：需求拆解与系统能力画像（VLA / AD / 语音）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第四章：数据总体策略与 30T token 配额（多语多模）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第五章 数据采集 I：网页文本与代码（合规）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章：数据采集 II：音频/语音（播客、公开课、语音数据集）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章 数据采集 III：视频（长/短视频、驾驶/具身）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter8.md] 数据采集 IV：图像</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 9 章 数据采集 V：3D（程序化优先）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十章：数据治理与质量度量（跨模态）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章 过滤与去脏：fastText 与小模型策略库</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 12 章：合成数据 I：教科书式文本/指令（Phi-3 风格）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十三章 合成数据 II：音频与语音</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 14 章 合成数据 III：视频与 VLA 自博弈（Agentic RL Self‑Play）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Tokenizer 设计：文本/音频/视频/图像/3D</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter16.md] 生成‑理解一体架构沿革</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter17.md] 模型架构：Qwen‑式自回归 Transformer（Dense / 先进 MoE，早期融合）</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter19.md] 训练配方：从 1B 到 10B 的生产级方案</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter19.md] 训练配方：从 1B 到 10B 的生产级方案</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 20 章 蒸馏与教师模型：Gemma‑式 Logits 蒸馏及多模态知识迁移</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter21.md] 对齐与中期训练（Instruction/Mid-training/偏好）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 22 章：评测与基准（多模/多语/VLA/驾驶/语音）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter23.md] 成本、运维与 MLOps</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 24 章：交付与复现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 25 章：安全、法律与合规</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter26.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter26.md] 项目管理与人员阵型：大型AI工程的“交响乐指挥法”</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter27.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">`[chapter27.md]` 常见陷阱与故障排查</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter28.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 A：配置与脚本模板（可复制）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="appendixA.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 A：配置与脚本模板（可复制）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="chapter19md-1b-10b">[chapter19.md] 训练配方：从 1B 到 10B 的生产级方案</h1>
<h3 id="_1">开篇段落</h3>
<p>本章是整个预训练项目的“总装图”与“操作手册”，我们将前述所有关于数据、架构和基建的理论知识，凝聚成一份可执行、可复现的生产级“训练配方”。学习本章，您将不仅获得为 1B Dense 和 10B MoE 两种规模的多模态大模型设定超参数的具体数值，更将深入理解这些数值背后的原理、权衡与规模化法则（scaling laws）。我们将庖丁解牛般剖析学习率、批尺寸、序列打包、数据混合、混合损失函数以及至关重要的吞吐量预估等核心环节。本章的目标是赋能 AI Scientist 设计出稳定高效的训练策略，并为 Infra 工程师提供清晰的性能目标与监控重点，确保这次昂贵而漫长的预训练任务能够精准、稳健地驶向终点。</p>
<h3 id="_2">文字论述</h3>
<h4 id="191-cosineone-cycle">19.1 学习率/批尺寸/采样调度（cosine/one-cycle）</h4>
<p>训练的成败始于优化器及其调度策略的精妙设计。对于参数量动辄上亿的大模型，任何微小的配置失误都可能导致灾难性的后果，如训练发散、收敛缓慢或陷入糟糕的局部最优。</p>
<ul>
<li>
<p><strong>优化器 (Optimizer)</strong>：我们坚定地选择 <strong>AdamW</strong>。相较于经典的 Adam，AdamW 将权重衰减（weight decay）从梯度更新中解耦，直接作用于权重本身，这在实践中被证明能带来更好的泛化性能和训练稳定性。</p>
<ul>
<li><strong><code>betas</code></strong>: 设为 <code>(0.9, 0.95)</code>。<code>beta1</code> 控制梯度的一阶矩（动量），<code>beta2</code> 控制二阶矩（自适应学习率）。将 <code>beta2</code> 设为 <code>0.95</code> 而非默认的 <code>0.999</code>，可以减少梯度的方差估计，使学习过程在处理大规模、高维度数据时更加平滑，有效抑制了训练初期的尖峰（spikes）。</li>
<li><strong><code>epsilon</code></strong>: <code>1e-8</code> 是一个标准的安全设置，防止在梯度二阶矩极小的情况下出现除零错误。</li>
<li><strong><code>weight_decay</code></strong>: 这是一个关键的正则化项，用于惩罚过大的权重以防止过拟合。其取值与模型规模和数据量强相关，通常在 <code>0.01</code> 到 <code>0.1</code> 之间。对于 10B 规模的模型，倾向于取较大的值（如 <code>0.1</code>）。</li>
</ul>
</li>
<li>
<p><strong>批尺寸 (Batch Size)</strong>：在分布式训练中，我们必须区分三个概念：</p>
<ol>
<li><strong>微批尺寸 (Micro-Batch Size)</strong>：单张 GPU 一次前向/后向计算处理的样本数。受限于单卡 80GB 显存，通常设为 4、8 或 16。</li>
<li><strong>梯度累积步数 (Gradient Accumulation Steps)</strong>：在执行一次优化器更新（<code>optimizer.step()</code>）前，累积多少个微批次的梯度。这是在不增加显存消耗的情况下，有效增大批尺寸的常用技巧。</li>
<li><strong>全局批尺寸 (Global Batch Size, GBS)</strong>：这是真正影响模型学习动态的参数，单位通常是 <strong>tokens</strong>。
    <code>GBS (tokens) = Micro-Batch Size × Seq Length × Grad Accumulation Steps × Data Parallel Size</code></li>
</ol>
<p><strong>Rule-of-Thumb</strong>: 在算力预算内，将 GBS 推向极限，通常目标在 <strong>2M 到 4M tokens</strong>。更大的 GBS 提供了更稳定的梯度估计，允许使用更高的学习率，从而可能加速收敛。然而，过大的 GBS 也可能导致模型收敛到泛化能力较差的“尖锐”最小值。</p>
</li>
<li>
<p><strong>学习率调度器 (Learning Rate Scheduler)</strong>：我们采用业界验证的黄金组合：<strong>带线性预热的余弦退火 (Cosine Decay with Linear Warmup)</strong>。</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">Peak</span><span class="w"> </span><span class="n">LR</span><span class="w"> </span><span class="o">+--------------+</span>
<span class="w">          </span><span class="o">/|</span><span class="w">              </span><span class="o">|</span><span class="err">\</span>
<span class="w">         </span><span class="o">/</span><span class="w"> </span><span class="o">|</span><span class="w">              </span><span class="o">|</span><span class="w"> </span><span class="err">\</span>
<span class="w">        </span><span class="o">/</span><span class="w">  </span><span class="o">|</span><span class="w">              </span><span class="o">|</span><span class="w">  </span><span class="err">\</span>
<span class="n">LR</span><span class="w">     </span><span class="o">/</span><span class="w">   </span><span class="o">|</span><span class="w">   </span><span class="n">Cosine</span><span class="w">     </span><span class="o">|</span><span class="w">   </span><span class="err">\</span>
<span class="w">      </span><span class="o">/</span><span class="w">    </span><span class="o">|</span><span class="w">   </span><span class="n">Decay</span><span class="w">      </span><span class="o">|</span><span class="w">    </span><span class="err">\</span>
<span class="w">     </span><span class="o">/</span><span class="w">     </span><span class="o">|</span><span class="w">              </span><span class="o">|</span><span class="w">     </span><span class="err">\</span>
<span class="w">    </span><span class="o">/</span><span class="w">      </span><span class="o">|</span><span class="w">              </span><span class="o">|</span><span class="w">      </span><span class="err">\</span>
<span class="o">---+-------+--------------+-------&gt;</span><span class="w"> </span><span class="nf">Min</span><span class="w"> </span><span class="n">LR</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="mf">0.1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Peak</span><span class="w"> </span><span class="n">LR</span><span class="p">)</span>
<span class="w">   </span><span class="o">|</span><span class="n">Warmup</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Main</span><span class="w"> </span><span class="n">Training</span><span class="o">|</span><span class="w"> </span><span class="n">Cool</span><span class="o">-</span><span class="n">down</span>
<span class="w">   </span><span class="o">&lt;------&gt;</span><span class="w"> </span><span class="o">&lt;------------&gt;</span>
<span class="w">            </span><span class="n">Total</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Steps</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">*</span>   **预热 (Warmup)**：在训练启动时，模型权重是随机的，梯度方向极不稳定。我们从一个极小的 LR（如峰值的 1%）开始，经过数千步线性增长到峰值 LR。这给了模型一个“缓冲期”，使其初步适应数据分布，避免早期发散。预热步数通常占总训练步数的 1% - 5%。对于 10T token / 4M GBS = 2.5M steps 的训练，预热 3k-5k 步是合理的。
<span class="k">*</span>   **峰值 LR (Peak LR)**：这是整个训练中学习率能达到的最高点。
    <span class="k">*</span>   **1B 模型**: <span class="sb">`3e-4`</span> 到 <span class="sb">`6e-4`</span>。这是一个较为激进但高效的范围。
    <span class="k">*</span>   **10B 模型**: <span class="sb">`1e-4`</span> 到 <span class="sb">`2e-4`</span>。模型越大，参数空间越复杂，通常需要更小的学习率来保证探索的稳定性。
<span class="k">*</span>   **退火 (Decay)**：预热结束后，LR 遵循余弦曲线平滑下降。这种策略使得训练的大部分时间都维持在较高的学习率以充分探索，而在训练后期逐降低学习率，帮助模型在已经找到的“盆地”中精细收敛，找到更优的点。
</code></pre></div>

<h4 id="192">19.2 长序列/多模态序列打包与高效填充（多摄时间对齐）</h4>
<p>GPU 是为大规模并行计算而生的，其效率在处理长序列时远高于处理大量填充（padding）的短序列。<strong>序列打包 (Sequence Packing)</strong> 是将 MFU（模型 FLOPs 利用率）从 50-60% 提升至 90% 以上的关键技术。</p>
<ul>
<li>
<p><strong>核心机制</strong>：将多个逻辑上独立的短序列在物理上拼接成一个接近 <code>max_seq_len</code> 的长序列。其灵魂在于一个精心构造的 <strong>注意力掩码 (Attention Mask)</strong>。这是一个二维矩阵，<code>mask[i, j]=1</code> 表示 token <code>i</code> 可以关注 token <code>j</code>，否则为 <code>0</code>。对于打包序列，该掩码呈现块对角结构，严格限制了注意力计算的范围，确保信息不会在独立样本间泄露。</p>
</li>
<li>
<p><strong>多模态打包的挑战与方案</strong>：</p>
<ol>
<li><strong>模态与结构边界</strong>：我们使用丰富的特殊 token 来界定结构。例如：<code>&lt;|video_start|&gt; &lt;|timestamp_1.0s|&gt; &lt;|cam_front|&gt; P...P &lt;|cam_left|&gt; P...P ... &lt;|video_end|&gt;</code>。这些特殊 token 不仅是分隔符，它们本身也拥有可学习的嵌入向量，向模型传递结构化的元信息。</li>
<li><strong>时间与空间对齐</strong>：对于 <strong>6-camera 环视视频</strong>，这是 VLA 和自动驾驶场景的命脉。数据加载器必须保证在打包时，同一时间戳 <code>t</code> 的 6 个视频帧（或 tubelet）、对应的音频片段、IMU 读数、文本字幕等被视为一个不可分割的“时间切片组”。打包器在拼接时，会优先将连续的时间切片组放在一起，以利于模型学习时序动态。</li>
<li><strong>异构 Token 流</strong>：一个打包好的序列可能是这样的混合体：</li>
</ol>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="p">[</span><span class="n">Text</span><span class="w"> </span><span class="n">Tokens</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">video_start</span><span class="o">|&gt;</span><span class="w"> </span><span class="p">[</span><span class="n">Vision</span><span class="w"> </span><span class="n">Patches</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Cam1</span><span class="p">@</span><span class="n">T1</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">Vision</span><span class="w"> </span><span class="n">Patches</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Cam2</span><span class="p">@</span><span class="n">T1</span><span class="p">]</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">audio_start</span><span class="o">|&gt;</span><span class="w"> </span><span class="p">[</span><span class="n">Audio</span><span class="w"> </span><span class="n">Codec</span><span class="w"> </span><span class="n">Tokens</span><span class="p">@</span><span class="n">T1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">eos</span><span class="o">|&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">bos</span><span class="o">&gt;</span><span class="w"> </span><span class="p">[</span><span class="n">Text</span><span class="w"> </span><span class="n">Tokens</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">Sample</span><span class="w"> </span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="p">...</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    数据加载器需要一个智能的“装箱”算法，动地组合不同长度、不同模态的样本，以最大化每个打包序列的有效 token 比例。
</code></pre></div>

<h4 id="193-coldmidhot">19.3 数据混合温度与阶段化（Cold→Mid→Hot）</h4>
<p>一次成功的预训练不是简单地将所有数据混合在一起然后一味地训练。我们采用一种精心设计的课程学习（Curriculum Learning）策略，通过动态调整数据混合比例，引导模型分阶段地掌握不同层次的能力。</p>
<ul>
<li>
<p><strong>阶段一：冷启动 (Cold Start)</strong> [前 1T token]</p>
<ul>
<li><strong>数据构成</strong>：以高质量、低噪声、结构化的“教科书”数据为主。包括：合成的 Phi-3 式文本、精选代码库、学术论文（arXiv）、高质量维基百科、清洗过的书籍。在多模态侧，则是高分辨率的图像-文本对、有清晰转录的播客和公开课。</li>
<li>
<p><strong>混合温度 (T)</strong>：较低 (e.g., <code>T=1.2</code>)。我们使用温度采样公式来决定从哪个数据集中抽样：
    $$ P(D_i) = \frac{n_i^{1/T}}{\sum_{j} n_j^{1/T}} $$
其中 <code>n_i</code> 是数据集 <code>D_i</code> 的大小。较低的 <code>T</code> 会放大规模效应，使得模型更频繁地看到那些我们认为质量最高的大型数据集。</p>
</li>
<li>
<p><strong>目标</strong>：构建模型的“世界观”骨架。让模型快速、无歧义地学习语言的语法结构、基础物理世界规律、跨模态的基本对应关系（例如，“狗”的文字、图片和声音）。</p>
</li>
</ul>
</li>
<li>
<p><strong>阶段二：中盘泛化 (Mid-training)</strong> [1T - 9T token]</p>
<ul>
<li><strong>数据构成</strong>：全面引入多样性。大规模、经过清洗过滤的网络抓取数据（如 Common Crawl 的一个子集）、YouTube 视频、社交媒体图片等成为主力。此时，数据的广度优先于深度。</li>
<li><strong>混合温度 (T)</strong>：调高 (e.g., <code>T=3.0</code> to <code>5.0</code>)。较高的温度会“压平”采样分布，让那些规模较小但内容独特的长尾数据集（如特定领域的论坛、方言语音库）有更多机会被模型看到。</li>
<li><strong>目标</strong>：用海量的真实世界数据填充模型的血肉，提升其在种噪声、风格和场景下的鲁棒性和泛化能力。</li>
</ul>
</li>
<li>
<p><strong>阶段三：热身冲刺 (Hot Finish)</strong> [最后 1T token]</p>
<ul>
<li><strong>数据构成</strong>：重新聚焦于高质量和目标领域数据。可以增加指令/对话数据集、VLA 轨迹数据、高质量驾驶视频的比例。</li>
<li><strong>混合温度 (T)</strong>：再次降低 (e.g., <code>T=1.5</code>)，以强化模型在特定任务格式和高价值知识上的记忆。</li>
<li><strong>目标</strong>：为后续的指令微调（SFT）和对齐阶段“预热”，使模型对下游任务的格式更敏感，减少对齐阶段的负担。</li>
</ul>
</li>
</ul>
<h4 id="194-infoncece">19.4 正负样本与 InfoNCE/CE 混合损失（多模）</h4>
<p>我们的模型是生成-理解一体的架构，因此损失函数也需要兼顾两方面的能力。</p>
<ol>
<li>
<p><strong>自回归交叉熵损失 (Autoregressive Cross-Entropy Loss)</strong>：这是模型生成能力的基础。对于一个多模态 token 序列 <code>S = {t_1, t_2, ..., t_L}</code>，损失函数是预测下一个 token 的负对数似然之和：
$$ \mathcal{L}_{\text{CE}} = - \sum_{i=1}^{L} \log P(t_i | t_{&lt;i}, \mathcal{C}) $$
其中 <code>C</code> 是上下文，包含了所有模态的输入。</p>
</li>
<li>
<p><strong>对比学习损失 (InfoNCE Loss)</strong>：为了强化跨模态表征的对齐，我们引入对比学习。具体做法是：从每个多模态样本中，提取出各个模态的整体表征（例如，通过一个特殊的 <code>[CLS]</code> token 或者对所有 token 的表征进行平均池化）。对于一个（视频 <code>V</code>, 文本 <code>T</code>）正样本对，我们在一个全局批次内将其余 <code>N-1</code> 个样本的文本 <code>T_j</code> 作为负样本。
$$ \mathcal{L}_{\text{InfoNCE}}(V, T) = - \log \frac{\exp(\text{sim}(E_V, E_T) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(E_V, E_{T_j}) / \tau)} $$
其中 <code>E</code> 是模态编码器输出的表征，<code>sim</code> 是余弦相似度，<code>τ</code> 是温度超参数（通常设为 0.07 左右），它控制着正负样本区分的难易程度。</p>
</li>
<li>
<p><strong>VLA 行动预测损失 (Action Prediction Loss)</strong>：对于包含行动（Action）数据的样本，我们对行动 token 施加额外的损失。如果行动是离散的（如“点击按钮 A”），则使用交叉熵损失。如果行动是连续的（如 <code>[steer: -0.2, throttle: 0.5]</code>），则使用 L2 回归损失。
$$ \mathcal{L}_{\text{Action}} = \frac{1}{M} \sum_{k=1}^{M} | a_k - \hat{a}_k |^2_2 $$
最终的总损失是三者的加权和：
$$ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \lambda_1 \cdot \mathcal{L}_{\text{InfoNCE}} + \lambda_2 \cdot \mathcal{L}_{\text{Action}} $$
权重 <code>λ1</code> 和 <code>λ2</code> 是需要仔细调整的超参数，用以平衡模型的生成流畅性、跨模态理解深度和行动执行精度。</p>
</li>
</ol>
<h4 id="195-w9">19.5 吞吐建模与算力-时长预算 <strong>[里程碑 W9]</strong></h4>
<p>在按下“启动”按钮前，一份精确的成本与时间预算是项目管理的核心。</p>
<ul>
<li>
<p><strong>核心性能指标</strong>：<strong>模型 FLOPs 利用率 (Model FLOPs Utilization, MFU)</strong>。它衡量了实际达到的有效计算速度与理论峰值的比值。</p>
<ul>
<li><strong>MFU</strong> = <code>实际观测到的 TFLOPS / 模型理论 TFLOPS</code></li>
<li>对于 A100/H100 上的大规模训练，一个经过优化的 Megatron-LM 实现，其 MFU 目标应在 <strong>45% - 55%</strong>。低于 40% 意味着存在严重的系统瓶颈（I/O、通信或软件开销）。</li>
</ul>
</li>
<li>
<p><strong>单 Token 计算量 (FLOPs/token)</strong>：</p>
<ul>
<li>Dense Transformer: <code>FLOPs ≈ 2 × N_params</code> (一次 FWD+BWD)</li>
<li>MoE Transformer: <code>FLOPs ≈ 2 × (N_shared + k × N_expert)</code>，其中 <code>k</code> 是激活的专家数。</li>
</ul>
</li>
<li>
<p><strong>10B MoE 模型训练时长预估 (示例)</strong>:</p>
<ul>
<li><strong>硬件</strong>: 256 × H100 80GB GPU. 单卡 FP8 理论峰值 ≈ 2000 TFLOPS.</li>
<li><strong>集群理论总算力</strong>: 256 × 2000 TFLOPS = 512 PFLOPS.</li>
<li><strong>目标 MFU</strong>: 50%. <strong>集群有效算力</strong>: 512 × 0.5 = 256 PFLOPS.</li>
<li><strong>模型</strong>: 10B MoE, 激活 2 个专家, 有效参数量约为 2.5B。</li>
<li><strong>FLOPs/token</strong>: <code>2 × 2.5B ≈ 5 GFLOPs</code>.</li>
<li><strong>总计算量</strong>: <code>10T tokens × 5 GFLOPs/token = 5 × 10^{12} × 5 × 10^9 = 2.5 × 10^{22} FLOPs = 25 ZFLOPs</code>.</li>
<li><strong>预估总时长 (秒)</strong>: <code>总计算 / 集群有效算力 = 2.5 × 10^{22} / (256 × 10^{15}) ≈ 9.76 × 10^4 秒</code>.</li>
<li><strong>预估总时长 (天)</strong>: <code>97600 / (3600 × 24) ≈ 11.3 天</code>. (这是纯计算时间，实际需考虑I/O、checkpoint、维护等开销，乘以 1.2-1.3 的 buffer 较为现实)。</li>
</ul>
</li>
</ul>
<p><strong>[里程碑 W9]</strong> 团队完成小规模（如 1B 模型跑 1T token）的吞吐测试，校准 MFU 达到 45% 以上，并基于此冻结 10T token 主训练的最终时长和成本预算。</p>
<h4 id="196-1b-dense-w10">19.6 1B Dense 端到端基线复现实验 <strong>[里程碑 W10]</strong></h4>
<p>在启动耗资巨大的 10B 模型训练前，用 1B 的 Dense 模型进行一次全流程的“带妆彩排”是不可或缺的风险控制环节。</p>
<ul>
<li>
<p><strong>彩排目标</strong>：</p>
<ol>
<li><strong>数据链路验证</strong>：从对象存储，经过数据加载、预处理、多模态打包，到送入 GPU，整个数据流是否通畅、高效、无误。</li>
<li><strong>软件栈集成测试</strong>：Megatron 的并行配置（TP/PP/DP）、FP8 训练（TransformerEngine）、检查点机制、监控系等是否协同工作正常。</li>
<li><strong>模型行为初步观察</strong>：损失曲线是否平稳下降？梯度范数是否在合理范围？是否存在数值不稳定问题？</li>
</ol>
</li>
<li>
<p><strong>1B Dense 基线配方 (参考)</strong>：
    | 超参数 | 建议值 | 备注 |</p>
<table>
<thead>
<tr>
<th style="text-align: left;">超参数</th>
<th style="text-align: left;">建议值</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Global Batch Size</td>
<td style="text-align: left;">2M tokens</td>
<td style="text-align: left;">在 256 卡上易于实现</td>
</tr>
<tr>
<td style="text-align: left;">Peak Learning Rate</td>
<td style="text-align: left;">5e-4</td>
<td style="text-align: left;">1B 规模模型的典型值</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Steps</td>
<td style="text-align: left;">3000 steps</td>
<td style="text-align: left;">约 10-15B tokens</td>
</tr>
<tr>
<td style="text-align: left;">Max Sequence Length</td>
<td style="text-align: left;">4096</td>
<td style="text-align: left;">平衡性能与显存</td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: left;">0.05</td>
<td style="text-align: left;">中等强度的正则化</td>
</tr>
<tr>
<td style="text-align: left;">Adam Betas</td>
<td style="text-align: left;">(0.9, 0.95)</td>
<td style="text-align: left;">稳定训练的标准配置</td>
</tr>
<tr>
<td style="text-align: left;">优化器状态精度</td>
<td style="text-align: left;">BF16</td>
<td style="text-align: left;">节省显存，性能几乎无损</td>
</tr>
</tbody>
</table>
</li>
</ul>
<p><strong>[里程碑 W10]</strong> 1B Dense 模型成功跑通 100B token 的短程训练。损失曲线符合预期，MFU 稳定在目标区间，无重大 bug。这次成功的试跑为 10B 模型的启动扫清了障碍。</p>
<h4 id="197-10b-moe-w12w18">19.7 10B 先进 MoE 主训练与监控面板 <strong>[里程碑 W12–W18]</strong></h4>
<p>这是项的核心战役。10B MoE 模型不仅规模更大，其稀疏激活的特性也带来了独特的挑战和监控需求。</p>
<ul>
<li>
<p><strong>10B MoE 配方 (参考)</strong>：
    | 超参数 | 建议值 | 备注 |</p>
<table>
<thead>
<tr>
<th style="text-align: left;">超参数</th>
<th style="text-align: left;">建议值</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Global Batch Size</td>
<td style="text-align: left;">4M tokens</td>
<td style="text-align: left;">充分利用集群规模</td>
</tr>
<tr>
<td style="text-align: left;">Peak Learning Rate</td>
<td style="text-align: left;">1.5e-4</td>
<td style="text-align: left;">更大模型，更小 LR 以求稳定</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Steps</td>
<td style="text-align: left;">5000 steps</td>
<td style="text-align: left;">预热更长，适应更复杂的模型</td>
</tr>
<tr>
<td style="text-align: left;">Max Sequence Length</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;">提升长上下文处理能力</td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">更强的正则化以应对更大容量</td>
</tr>
<tr>
<td style="text-align: left;">Adam Betas</td>
<td style="text-align: left;">(0.9, 0.95)</td>
<td style="text-align: left;">保持不变</td>
</tr>
<tr>
<td style="text-align: left;">Expert Parallelism</td>
<td style="text-align: left;">8 or 16</td>
<td style="text-align: left;">根据 MoE 专家数和网络拓扑决定</td>
</tr>
<tr>
<td style="text-align: left;">Capacity Factor</td>
<td style="text-align: left;">1.25</td>
<td style="text-align: left;">允许 25% 的专家容量冗余</td>
</tr>
</tbody>
</table>
</li>
<li>
<p><strong>MoE 专属监控仪表盘 (必备)</strong>：</p>
<ol>
<li><strong>专家负载均衡 (Expert Load Balancing)</strong>：以直方图形式实时展示每个专家处理的 token 数量。<ul>
<li><strong>健康状态</strong>: 所有专家的负载大致均匀，呈平分布。</li>
<li><strong>异常状态</strong>: “路由坍塌”，少数几个专家处理了绝大部分 token，形成尖锐的峰。</li>
</ul>
</li>
<li><strong>路由辅助损失 (Auxiliary Load Balancing Loss)</strong>：监控该损失项。它应在训练初期快速下降，然后在一个较低的水平上平稳波动。若持续上升，说明路由机制正在失效。</li>
<li><strong>门控网络激活统计 (Gating Activations)</strong>：监控门控网络输出的权重（logits）的均值和方差。这可以揭示路由器的“决策健康度”。</li>
<li><strong>Token 掉落率 (Token Drop Rate)</strong>：如果专家容量（Capacity）不足，部分 token 会被“丢弃”。这个比率必须持续保持在接近 0%（如 &lt;0.1%）。任何显著的掉落都意味着计算资源的浪费和模型性能的损失。</li>
</ol>
</li>
</ul>
<p><strong>[里程碑 W12–W18]</strong> 10B MoE 模型主预训练正式启动并按计划推进。7x24 小时的值班团队紧盯监控面板，确保在长达数周的训练周期内，系统稳定运行，各项 MoE 指标健康，最终完成 10T token 的训练目标。</p>
<hr />
<h3 id="_3">本章小结</h3>
<p>本章提供了一份详尽的、从 1B 到 10B 规模的多模态大模型生产级训练手册。</p>
<ul>
<li><strong>优化与调度</strong>：我们确立了以 AdamW 和余弦退火学习率调度器为核心的优化策略，并深入探讨了 <code>beta2</code>、<code>weight_decay</code> 等关键参数的选择依据。</li>
<li><strong>效率与性能</strong>：通过精细的多模态序列打包技术，我们解决了异构数据带来的效率挑战，将硬件利用率推向极致。时间对齐是 VLA 场景成功的基石。</li>
<li><strong>课程化学习</strong>：“冷→中→热”三阶段数据混合策略，通过动态调整采样温度，实现了从构建基础到泛化再到强化的科学学习路径。</li>
<li><strong>多目标损失</strong>：我们设计了融合交叉熵、InfoNCE 对比学习和行动预测的混合损失函数，确保模型在生成、理解、对齐和行动四个维度上均衡发展。</li>
<li><strong>工程化保障</strong>：强调了基于 MFU 的精确吞吐建模对于项目预算和时管理的重要性，并通过 1B 基线试跑作为风险缓释的关键步骤。对于 10B MoE 模型，我们定义了其特有的监控指标，为大规模训练的稳定性保驾护航。</li>
</ul>
<hr />
<h3 id="gotchas">常见陷阱与错误 (Gotchas)</h3>
<ol>
<li>
<p><strong>学习率与批尺寸的“经典”误配</strong>：将小模型（如 BERT）的 <code>5e-5</code> LR 错误地应用到 GBS 为 4M 的大模型上，导致训练“停滞不前”。反之，在未充分预热的情况下使用过高的 LR，导致 loss 在前几百步就变成 <code>NaN</code>。</p>
<ul>
<li><strong>调试技巧</strong>：执行“LR Range Test”：用一个极小的 LR 开始，在几百步内指数级地增大 LR，绘制 loss-LR 曲线，找到 loss 开始下降最快的 LR 区域作为峰值 LR 的参考。始终确保预热足够长。</li>
</ul>
</li>
<li>
<p><strong>序列打包中的“幽灵”注意力</strong>：注意力掩码实现中的一个 off-by-one 错误，或对特殊 token 的处理不当，可能导致一个样本的最后一个 token 意外地看到了下一个样本的第一个 token。</p>
<ul>
<li><strong>调试技巧</strong>：编写一个独立的脚本，加载数据并生成一个微批次的数据，然后将打包后的 <code>input_ids</code> 和 <code>attention_mask</code> 可视化。打印出掩码矩阵，并手动抽查几个关键位置（如样本边界、模态边界），确认其值为 0 或 1 是否符合逻辑。</li>
</ul>
</li>
<li>
<p><strong>MoE 路由坍塌的“死亡螺旋”</strong>：由于某些原因（如初始化不当、数据分布剧变），少数专家开始获得略多于平均的 token。这使得它们被训练得更好，从而吸引更多 token，形成正反馈，最终导致负载严重失衡。</p>
<ul>
<li><strong>调试技巧</strong>：除了监控负载均衡损失，还要监控路由门控的梯度范数。如果某些专家的门控梯度持续远大于其他专家，说明可能出现了问题。可以尝试引入门控噪声（Gating Noise）或增加负载均衡损失的权重 <code>λ_aux</code> 来打破这种正反馈。</li>
</ul>
</li>
<li>
<p><strong>跨模态时间戳的“微秒级”漂移</strong>：在数据预处理管道中，由于不同模态文件的时间戳精度不同（例如，视频帧是毫秒级，IMU 是微秒级），或者处理软件的浮点数误差累积，导致原本对齐的数据产生微小的时间偏差。</p>
<ul>
<li><strong>调试技巧</strong>：在数据样本进入打包器之前，设立一个严格的“同步检查点”。将所有模态数据的时间戳转换为统一的整数单位（如纳秒），并断言（assert）同一时间切片组内的所有数据项时间戳误差小于一个阈值（如 1ms）。</li>
</ul>
</li>
<li>
<p><strong>检查点 I/O 风暴</strong>：在 256 个节点上同时向一个共享文件系统（如 NFS）写入巨大的检查点文件，可能瞬间打垮文件系统的元数据服务器，导致整个集群 I/O 卡死。</p>
<ul>
<li><strong>调试技巧</strong>：采用分层、分片的异步保存策略。每个节点先将自己的分片写入本地高速存储（如 NVMe SSD），然后由一个后台进程池以受控的速率将这些分片上传到持久化的对象存储。这可以平滑 I/O 峰值，降低对共享资源的冲击。</li>
</ul>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter17.html" class="nav-link prev">← [chapter17.md] 模型架构：Qwen‑式自回归 Transformer（Dense / 先进 MoE，早期融合）</a><a href="chapter19.html" class="nav-link next">[chapter19.md] 训练配方：从 1B 到 10B 的生产级方案 →</a></nav>
        </main>
    </div>
</body>
</html>