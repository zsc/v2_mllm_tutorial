<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>[chapter21.md] 对齐与中期训练（Instruction/Mid-training/偏好）</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">面向 Vision‑Language‑Action（VLA）、自动驾驶/具身与语音交互的多模态大模型预训练教程（公开版）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter1.md] 前言、范围与读者指南</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 2 章：全局时间线与里程碑（W0–W26）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第三章：需求拆解与系统能力画像（VLA / AD / 语音）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第四章：数据总体策略与 30T token 配额（多语多模）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第五章 数据采集 I：网页文本与代码（合规）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章：数据采集 II：音频/语音（播客、公开课、语音数据集）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章 数据采集 III：视频（长/短视频、驾驶/具身）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter8.md] 数据采集 IV：图像</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 9 章 数据采集 V：3D（程序化优先）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十章：数据治理与质量度量（跨模态）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章 过滤与去脏：fastText 与小模型策略库</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 12 章：合成数据 I：教科书式文本/指令（Phi-3 风格）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十三章 合成数据 II：音频与语音</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 14 章 合成数据 III：视频与 VLA 自博弈（Agentic RL Self‑Play）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Tokenizer 设计：文本/音频/视频/图像/3D</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter16.md] 生成‑理解一体架构沿革</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter17.md] 模型架构：Qwen‑式自回归 Transformer（Dense / 先进 MoE，早期融合）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter19.md] 训练配方：从 1B 到 10B 的生产级方案</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter19.md] 训练配方：从 1B 到 10B 的生产级方案</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 20 章 蒸馏与教师模型：Gemma‑式 Logits 蒸馏及多模态知识迁移</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter21.md] 对齐与中期训练（Instruction/Mid-training/偏好）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 22 章：评测与基准（多模/多语/VLA/驾驶/语音）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter23.md] 成本、运维与 MLOps</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 24 章：交付与复现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 25 章：安全、法律与合规</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter26.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">[chapter26.md] 项目管理与人员阵型：大型AI工程的“交响乐指挥法”</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter27.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">`[chapter27.md]` 常见陷阱与故障排查</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter28.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 A：配置与脚本模板（可复制）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="appendixA.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">附录 A：配置与脚本模板（可复制）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="chapter21md-instructionmid-training">[chapter21.md] 对齐与中期训练（Instruction/Mid-training/偏好）</h1>
<h2 id="210">21.0 开篇段落：从“博学”到“可用”的最后一公里</h2>
<p>预训练阶段，我们的模型通过在 30T token 的海量多模态数据上进行自监督学习，构建了一个强大的世界模型。它能理解像素、声波、文字、几何结构之间的复杂关联，堪称“博学”。然而，这种“博学”是被动的，模型的目标函数仅仅是预测下一个 token。它并不知道人类的意图，不理解对话的合作原则，更不具备在自动驾驶等高风险场景中做出符合安全、伦理与法规的决策能力。</p>
<p>本章将聚焦于“对齐”（Alignment）与“中期训练”（Mid-training），这是连接“博学”与“可用”桥梁，是项目成败的“最后一公里”。我们将深入探讨如何将一个通用的基础模型（Base Model）雕琢成一个有用（Helpful）、诚实（Honest）且无害（Harmless）的智能体。我们将从指令微调（SFT）开始，教会模型理解并执行多模态指令；接着深入语音和行动这两个特殊领域的对齐策略；最后，我们将探讨如何通过直接偏好优化（DPO）等先进技术，让模型学习人类的复杂偏好，从而真正满足 VLA、自动驾驶和语音交互的生产级需求。</p>
<p><strong>学习目标</strong>:</p>
<ol>
<li>深入理解多模态指令微调（SFT）的数据生态、格式设计哲学与规模化生产流程。</li>
<li>掌握语音交互对齐中的延迟、打断、韵律和轮次管理等关键技术。</li>
<li>系统学习具身行动对齐的范式，从行为克隆（BC）的陷阱到离线强化学习（Offline RL）的机遇。</li>
<li>精通直接偏好优化（DPO）的数学原理与工程实践，并了解其在安全约束下的应，如 Constitutional AI。</li>
<li>识别并规避对齐过程中的核心挑战，如灾难性遗忘、奖励“黑客”和对齐税。</li>
</ol>
<hr />
<h2 id="211">21.1 指令数据构造与多模态对话格式</h2>
<p>监督微调（Supervised Fine-Tuning, SFT）是模型对齐的第一步，其核心目标是教会模型以对话形式遵循指令。对于我们的 VLA 模型，这意味着指令和响应本身就是多模态的。</p>
<h3 id="2111">21.1.1 统一对话格式的哲学</h3>
<p>我们将所有交互都抽象为一个统一的、结构化的对话序列。这种格式不仅是模型的输入/输出规范，更是我们注入领域知识、引导模型思考、增强其可解释性的主要手段。</p>
<div class="codehilite"><pre><span></span><code><span class="k">[SYSTEM]</span>
<span class="na">&lt;|system|&gt; 你是一个多模T态驾驶助手，安全第一，遵守交通法规，并能与乘客进行语音交互。</span>
<span class="k">[USER]</span>
<span class="na">&lt;|observation|&gt;</span>
<span class="w">  </span><span class="na">&lt;|camera_pose id</span><span class="o">=</span><span class="s">&quot;1&quot; pos=&quot;[x,y,z]&quot; rot=&quot;[q_w,q_x,q_y,q_z]&quot;|&gt; &lt;|video_cam1_t0|&gt;...&lt;|video_cam1_tN|&gt;</span>
<span class="w">  </span><span class="na">&lt;|camera_pose id</span><span class="o">=</span><span class="s">&quot;2&quot; pos=&quot;...&quot; rot=&quot;...&quot;|&gt; &lt;|video_cam2_t0|&gt;...&lt;|video_cam2_tN|&gt;</span>
<span class="w">  </span><span class="na">...</span>
<span class="w">  </span><span class="na">&lt;|camera_pose id</span><span class="o">=</span><span class="s">&quot;6&quot; pos=&quot;...&quot; rot=&quot;...&quot;|&gt; &lt;|video_cam6_t0|&gt;...&lt;|video_cam6_tN|&gt;</span>
<span class="w">  </span><span class="na">&lt;|audio|&gt; &lt;|audio_t0|&gt;...&lt;|audio_tM|&gt; (乘客的语音指令)</span>
<span class="na">&lt;|text|&gt; &quot;前方路况如何？感觉有点复杂，我应该怎么并进去？&quot;</span>
<span class="na">&lt;|ipa|&gt; &quot;fɔŋ²¹⁴ tɕʰiɛn²¹⁴ lu⁵¹ kʰuaŋ⁵¹ ʐu³⁵ xɤ³⁵? kan³⁵ tɕyɛ³⁵ iou²¹⁴ tiɛn³⁵ fu⁵¹ tsa³⁵, uo²¹⁴ iŋ¹ kai¹ tsən³⁵ mə pɪŋ⁵¹ tɕɪn⁵¹ tɕʰy⁵¹?&quot;</span>
<span class="k">[ASSISTANT]</span>
<span class="na">&lt;|thought|&gt;</span>

<span class="w">  </span><span class="na">1.  **解析用户意图**</span><span class="o">:</span><span class="w"> </span><span class="s">用户感到不确定，询问前方路况并请求并线策略。</span>
<span class="w">  </span><span class="na">2.  **感知分析**</span><span class="o">:</span>
<span class="w">      </span><span class="na">-   **视频(Cam 1, 2)**</span><span class="o">:</span><span class="w"> </span><span class="s">前方主路车流量大，车速约 60 km/h。</span>
<span class="w">      </span><span class="na">-   **视频(Cam 3, 4 - 侧方)**</span><span class="o">:</span><span class="w"> </span><span class="s">左侧车道后方 50 米处有一辆卡车正在快速接近，速度差较大，不宜立即并线。</span>
<span class="w">      </span><span class="na">-   **视频(Cam 5 - 右后方)**</span><span class="o">:</span><span class="w"> </span><span class="s">右后方车道有足够空间。</span>
<span class="w">      </span><span class="na">-   **语音分析**</span><span class="o">:</span><span class="w"> </span><span class="s">用户语气中带有轻微的焦虑情绪。</span>
<span class="w">  </span><span class="na">3.  **决策推理**</span><span class="o">:</span><span class="w"> </span><span class="s">基于“安全第一”原，当前最佳策略是：减速等待，让卡车通过，然后寻找下一个安全的并线窗口。需要用安抚性的语言回应用户。</span>
<span class="w">  </span><span class="na">4.  **行动规划**</span><span class="o">:</span><span class="w"> </span><span class="s">生成一个平滑的减速和车道保持的动作序列。</span>
<span class="na">&lt;/|thought|&gt;</span>
<span class="na">&lt;|text|&gt; &quot;好的，请放心。前方主路车流较快，特别是左后方有一辆卡车正在接近。为了安全，我们先稍微减速保持车道，等它通过后，我们再找机会汇入。&quot;</span>
<span class="na">&lt;|action|&gt;</span>
<span class="w">  </span><span class="na">&lt;|control type</span><span class="o">=</span><span class="s">&quot;lon&quot; value=&quot;-0.5m/s^2&quot; duration=&quot;3s&quot;|&gt;</span>
<span class="w">  </span><span class="na">&lt;|control type</span><span class="o">=</span><span class="s">&quot;lat&quot; value=&quot;lane_keeping&quot;|&gt;</span>
</code></pre></div>

<p><strong>设计要点深度解析</strong>:</p>
<ol>
<li><strong><code>&lt;|thought|&gt;</code> 思维链 (CoT)</strong>: 这是提升模型复杂推理能力和可解释性的关键。在训练时，我们强制模型生成推理步骤。这不仅让模型学习了如何“思考”，也为我们调试模型提供了宝贵的中间过程。高质量的 CoT 数据通常需要由领域专家（如资深驾驶员、AI 科学家）撰写或审核。</li>
<li><strong>多模态特殊 Token</strong>:<ul>
<li><code>&lt;|camera_pose ...|&gt;</code>: 明确注入相机外参，让模型理解多摄像头的空间关系，这是实现 3D 感知和环视理解的基础。</li>
<li><code>&lt;|ipa|&gt;</code>: 引入国际音标（IPA）标注，让模型不仅能识别文本，还能理解发音的细微差别，这对处理方言、少数语种和口音至关重要。</li>
<li><code>&lt;|action|&gt;</code>/<code>&lt;|control|&gt;</code>: 将模型的行动输出结构化、离散化。这比直接回归连续值更稳定，也更容易与车辆的控制模块接口。</li>
</ul>
</li>
<li><strong>数据生态系统</strong>: 高质量SFT数据从何而来？<ul>
<li><strong>人类专家标注</strong>: 针对自动驾驶、医疗等高风险领域，这是最可靠的方式。成本高昂，但不可或缺。</li>
<li><strong>强模型蒸馏</strong>: 使用 GPT-4V 或其他顶尖模型，根据我们的格式规范生成海量的指令-响应对。需要设计精巧的 prompt 并进行严格的质量过滤。</li>
<li><strong>开源数据集重格式化</strong>: 将现有的学术数据集（如 COCO, VQA, Ego4D）转换成我们的统一对话格式。</li>
<li><strong>样本挖掘</strong>: 专门构造模型容易出错的场景，例如混淆指令、伦理困境、传感器异常等，并提供正确的“拒绝回答”或“请求澄清”的范例。</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>经验法则 (Rule-of-Thumb):</strong>
SFT 数据集的构建应遵循“分层多样性”原则。首先确保覆盖所有核心能力（L1层多样性：VQA, ASR, Action Prediction...），然后针对每个能力，穷举不同的场景和提问方式（L2层多样性：雨天/夜间的驾驶决策，带有口音的语音指令...），最后在表达层面进行改写和扩充（L3层多样性）。SFT 阶段投入 100k-1M 条高质量、多样化的样本是 10B 级别模型的常见配置。</p>
</blockquote>
<hr />
<h2 id="212">21.2 语音交互对齐：优化实时体验</h2>
<p>语音交互的对齐核心在于“流动性”，即模仿人类对话的自然节奏和实时反馈。</p>
<ol>
<li>
<p><strong>延迟与流式生成 (Streaming)</strong>:</p>
<ul>
<li><strong>挑战</strong>: 用户说完话后，模型需要时间进行 <code>ASR -&gt; NLU -&gt; Policy -&gt; NLG -&gt; TTS</code> 的完整流程，导恼人的延迟。</li>
<li><strong>对齐策略</strong>: 训练模型具备流式生成的能力。即模型在完全想好整个句子之前，就能开始生成开头的词语。这需要在训练时模拟这种“边想边说”的过程，例如，将完整的 CoT 和响应序列，切分成多个小块进行预测。同时，可以引入一个与响应启动时间（Time-to-First-Token）相关的惩罚项。</li>
</ul>
</li>
<li>
<p><strong>打断与轮次管理 (Barge-in &amp; Turn-taking)</strong>:</p>
<ul>
<li><strong>挑战</strong>: 对话不是独白。模型需要知道何时倾听，何时说话。</li>
<li><strong>对齐策略</strong>: 在训练数据中显式地建模这一过程。将实时的 VAD (Voice Activity Detection) 信号作为一个特殊的输入 token <code>&lt;|vad_on|&gt; / &lt;|vad_off|&gt;</code>。当模型正在生成响应时，如果输入流中出现 <code>&lt;|vad_on|&gt;</code>，则正确的目标输出应立即切换为 <code>&lt;|interrupt_and_listen|&gt;</code> 这样的特殊 token。</li>
</ul>
</li>
<li>
<p><strong>韵律与情感 (Prosody &amp; Emotion)</strong>:</p>
<ul>
<li><strong>挑战</strong>: 机械的语调会破坏用户体验。</li>
<li><strong>对齐策略</strong>: 这超越了纯文本。一种方法是引入韵律控制 token，如 <code>&lt;|pitch=high|&gt; &lt;|rate=fast|&gt;</code>，并在训练时由声学模型提供监督信号。另一种更先进的方法是，让模型直接预测离散的声学 token（来自神经编解码器，如 EnCodec），从而实现文本和韵律的一体化生成。偏好数据中应包含对“声音是否自然/富有情感”的评价。</li>
</ul>
</li>
</ol>
<hr />
<h2 id="213">21.3 行动对齐：从模仿到决策</h2>
<p>行动对齐的目标是让模型输出安全、有效、平滑的动作序列。</p>
<ol>
<li>
<p><strong>行为克隆 (Behavioral Cloning, BC) 及其局限</strong>:</p>
<ul>
<li><strong>核心</strong>: 将专家（人类驾驶员）的 <code>(Observation, Action)</code> 数据对作为监督信号进行模仿学习。这是最直接的起点。</li>
<li><strong>陷阱</strong>:<ul>
<li><strong>因果混淆 (Causal Confusion)</strong>: 模型可能学到虚假的相关性。例如，模型发现每次踩刹车时，前方的刹车灯都会亮起，于是错误地认为“点亮自己的刹车灯”是导致前车减速原因。</li>
<li><strong>协变量漂移 (Covariate Shift)</strong>: 由于模型的不完美，它可能会进入一个与训练数据分布略有不同的状态（e.g., 车辆稍微偏离车道中心）。由于训练数据中没有覆盖这种“次优状态”下如何纠正的样本，模型可能会束手无策，导致误差累积，最终完全偏离。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>超越简单 BC</strong>:</p>
<ul>
<li><strong>DAgger (Dataset Aggregation)</strong>: 一种经典的解决协变量漂移的方法。让模型在模拟器中运行，当它犯错时，由人类专家接管并提供正确的操作。将这些 <code>(模型犯错的Observation, 专家纠正的Action)</code> 数据点加入到训练集中，进行下一轮迭代。</li>
<li><strong>目标条件 BC (Goal-Conditioned BC)</strong>: 不仅模仿动作，还让模型理解动作背后的“意图”。输入变为 <code>(Observation, Goal)</code>，输出为 <code>Action</code>。例如，<code>Goal</code> 可以是“在下一个路口左转”。这大大增强了模型的泛化能力。</li>
<li><strong>扩散策略 (Diffusion Policies)</strong>: 动作序列的生成建模为一个从噪声到动作的去噪过程。这种方法在处理高维、连续的动作空间时表现出色，能够生成更平滑、更多样的轨迹。</li>
</ul>
</li>
<li>
<p><strong>离线强化学习 (Offline Reinforcement Learning)</strong>:</p>
<ul>
<li><strong>动机</strong>: BC 只能模仿“看到过的”行为，无法超越专家。RL 有潜力发现比专家更优的策略。但在线 RL（在真实环境中试错）对于自动驾驶来说过于危险和昂贵。</li>
<li><strong>核心思想</strong>: 从一个固定的、离线的专家数据集中学习。关键挑战是如何在不与环境交互的情况下，安全地评估和优化策略，避免对数据集中未见过的 <code>(state, action)</code> 对进行过高的价值估计。</li>
<li><strong>代表算法</strong>:<ul>
<li><strong>CQL (Conservative Q-Learning)</strong>: 在标准的 Q-Learning 损失上增加一个正则项，惩罚在数据集分布之外的 <code>(state, action)</code> 对的 Q 值，同时推动数据集中存在的 <code>(state, action)</code> 对的 Q 值。这使得学习过程更加保守”和稳定。</li>
<li><strong>IQL (Implicit Q-Learning)</strong>: 避免对未见过的动作进行显式查询，而是通过分位数回归等技术隐式地学习 Q 函数，表现稳健。</li>
</ul>
</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>经验法则 (Rule-of-Thumb):</strong>
行动对齐的路径应为：<strong>高质量 BC 基线 → DAgger 或 Goal-Conditioned BC 提升鲁棒性 → 谨慎引入 Offline RL 探索超越模仿的可能性</strong>。在 1B 模型上跑通 BC 和 DAgger 流程，在 10B 模型上再尝试引入 Offline RL。</p>
</blockquote>
<hr />
<h2 id="214">21.4 轻量偏好优化与安全约束</h2>
<p>SFT 教会模型“能做什么”，偏好优化教会模型“应该做什么”。</p>
<ol>
<li>
<p><strong>DPO (Direct Preference Optimization) 深度解析</strong>:</p>
<ul>
<li><strong>背景</strong>: 传统的 RLHF 需要训练一个独立的奖励模型，再用 PPO 等强化学习算法去优化，流程复杂且不稳定。DPO 将这两个步骤合二为一。</li>
<li>
<p><strong>数学原理</strong>: DPO 的巧妙之处在于，它发现奖励模型和最优策略之间存在一个解析关系。因此，可以直接通过一个简单的分类损失来优化策略，而无需显式拟合奖励。
    $ \mathcal{L}_{DPO}(\pi_{\theta}; \pi_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma(\beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_{\text{ref}}(y_l|x)}) \right] $</p>
<ul>
<li><strong>$(x, y_w, y_l)$</strong>: 一个偏好数据样本，包含指令 $x$、被偏好的响应 $y_w$ (chosen) 和被拒绝的响应 $y_l$ (rejected)。</li>
<li><strong>$\pi_{\theta}$</strong>: 我们要优化的模型。</li>
<li><strong>$\pi_{\text{ref}}$</strong>: SFT 阶段结束后的模型快照，作为参考。它的作用是防止 $\pi_{\theta}$ 为了迎合偏好而偏离原始预训练的数据分布太远，从而导致能力退化（灾难性遗忘）。</li>
<li><strong>$\beta$</strong>: 温度参数，控制我们对参考策略的偏离程度。$\beta$ 越高，模型越倾向于匹配偏好数据，但也可能牺牲多样性。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>多模态偏好数据</strong>:</p>
<ul>
<li><strong>语音</strong>: <code>(指令, 更自然的语音响应, 更生硬的语音响应)</code></li>
<li><strong>驾驶</strong>: <code>(路况视频, 更平稳安全的轨迹, 更激进危险的轨迹)</code></li>
<li><strong>VLA</strong>: <code>(场景+指令, 准确执行并提供解释的视频, 执行错误或无响应的视频)</code></li>
</ul>
</li>
<li>
<p><strong>安全对齐与 Constitutional AI</strong>:</p>
<ul>
<li><strong>挑战</strong>: 仅仅依赖人类标注来覆盖所有安全场景是不现实的。</li>
<li><strong>Constitutional AI</strong>: 这是一种扩展偏好学习的强大框架。<ol>
<li><strong>定义原则 (Constitution)</strong>: 制定一套指导模型行为的基本原则，例如“不提供有害建议”、“尊重用户隐私”、“驾驶时遵守交通法规”等。</li>
<li><strong>自我批判与修正</strong>: 让 SFT 模型对一个有害指令生成一个初始响应。然后，让模型根据“原则”来批判这个响应，并重写一个更安全、更合规的响应。</li>
<li><strong>生成偏好数据</strong>: 初始的有害响应成为 $y_l$，修正后的安全响应成为 $y_w$。这样可以大规模地、自化地生成用于安全对齐的偏好数据。</li>
</ol>
</li>
</ul>
</li>
</ol>
<hr />
<h2 id="215">21.5 本章小结</h2>
<p>本章系统地阐述了将一个强大的预训练模型转化为一个可靠、安全、有用的多模态智能体的对齐全过程。</p>
<ul>
<li>我们始于 <strong>SFT</strong>，通过设计精巧的统一对话格式和构建分层多样的数据集，为模型注入了遵循指令和结构化思考的能力。</li>
<li>接着，我们针对 <strong>语音交互</strong> 和 <strong>具身行动</strong> 这两个高度动态的场景，探讨了特定的对齐策略，从流式生成到离线强化学习，解决了实时性和安全性的核心痛点。</li>
<li>最后，我们深入研究了以 <strong>DPO</strong> 为代表的偏好优化技术，它能让模型学习人类细致入微的偏好，并通过 <strong>Constitutional AI</strong> 等方法系统性地增强模型的安全性。</li>
<li>对齐是一个精细的雕琢过程，它决定了模型的最终形态和用户体验，其重要性不亚于预训练本身。</li>
</ul>
<hr />
<h2 id="216-gotchas">21.6 常见陷阱与错误 (Gotchas)</h2>
<ol>
<li>
<p><strong>灾难性遗忘 (Catastrophic Forgetting)</strong></p>
<ul>
<li><strong>问题</strong>: 在 SFT/DPO 阶段，模型过度拟合对齐数据，导致其在预训练阶段学到的通用知识（如事实性问答、代码能力）显著衰退。这种能力的下降被称为“对齐税”（Alignment Tax）。</li>
<li><strong>调试与修复</strong>:<ul>
<li><strong>数据混合</strong>: 在 SFT 数据中混入 5-10% 的高质量预训练数据。</li>
<li><strong>使用 <code>π_ref</code></strong>: DPO 中的参考模型 <code>π_ref</code> 本身就是一种防止过度偏离的正则化。</li>
<li><strong>LoRA 等 PEFT 方法</strong>: 只微调模型的一小部分参数，可以有效保留大部分预训练知识。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>奖励“黑客” (Reward Hacking)</strong></p>
<ul>
<li><strong>问题</strong>: 在偏好学习中，模型找到了一个“捷径”来获得高偏好分数，但实际上并没有真正理解或满足用户的意图。例如，模型发现冗长、礼貌的回答更容易被标注为“偏好”，于是开始生成大量无关紧要的客套话，而忽略了核心问题。</li>
<li><strong>调试与修复</strong>:<ul>
<li><strong>多样化标注团队</strong>: 确保标注员背景多样，避免单一偏好主导。</li>
<li><strong>迭代式红队测试 (Red-Teaming)</strong>: 专门找人或用另一个 AI 来攻击模型，寻找奖励“黑客”的漏洞，并将这些案例加入到偏好数据集中。</li>
<li><strong>更精细的奖励设计</strong>: 在奖励模型训练（如果是RLHF）或偏好标注指南中，明确指出简洁性、信息量等也是重要的评估维度。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>SFT/偏好数据中的偏见放大</strong></p>
<ul>
<li><strong>问题</strong>: 标注员的社会、文化偏见会不知不觉地滲透到对齐数据中。模型在学习这些数据后，不仅会复现这些偏见，甚至会将其放大。</li>
<li><strong>调试与修复</strong>:<ul>
<li><strong>制定详尽的标注指南</strong>: 明确要求标注员保持中立、客观，并提供处理敏感话题的具体指导。</li>
<li><strong>偏见审计</strong>: 使用专门的基准测试集（如 BOLD, Bias in Open-domain Language Generation）来评估模型的偏见水平。</li>
<li><strong>对标注员进行无意识偏见培训</strong>。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>行动空间离散化误差 (Action Discretization Error)</strong></p>
<ul>
<li><strong>问题</strong>: 在自动驾驶中，我们将连续的控制信号（如方向盘转角 <code>[-270, 270]</code> 度）离散化为有限的 token（如 <code>-5</code>, <code>-2</code>, <code>0</code>, <code>2</code>, <code>5</code> 度）。这可能导致模型无法做出精细的微调，或者在两个离散点之间震荡，产生不平顺的驾驶行为。</li>
<li><strong>调试与修复</strong>:<ul>
<li><strong>增加离散化的粒度</strong>: 更细的 action bin，但会增加词表大小和预测难度。</li>
<li><strong>混合方法</strong>: 预测一个粗粒度的 action bin，并同时回归一个在该 bin 内的残差值（residual value）。</li>
<li><strong>评估平顺性指标</strong>: 在评测中加入如加加速度（Jerk）等指标，来量化轨迹的平顺性。</li>
</ul>
</li>
</ul>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter20.html" class="nav-link prev">← 第 20 章 蒸馏与教师模型：Gemma‑式 Logits 蒸馏及多模态知识迁移</a><a href="chapter22.html" class="nav-link next">第 22 章：评测与基准（多模/多语/VLA/驾驶/语音） →</a></nav>
        </main>
    </div>
</body>
</html>